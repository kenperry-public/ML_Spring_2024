{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)\n",
    "\n",
    "<!--- #include (README.md) --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "**Plan**\n",
    "- Motivate Machine Learning\n",
    "- Introduce notation used throughout course\n",
    "- Plan for initial lectures\n",
    "    - *What*: Introduce, motivate a model\n",
    "    - *How*:  How to use a model: function signature, code (API)\n",
    "    - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "\n",
    "        \n",
    "- [Course Overview](Course_overview_NYU.ipynb)\n",
    "\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)\n",
    "\n",
    "**Week 2 (early start)**\n",
    "\n",
    "**Plan**\n",
    "- Introduce a model for the Regression task: Linear Regression\n",
    "- Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    \n",
    "**The Recipe for Machine Learning, illustrated with the Linear Regression model**  \n",
    "- [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We will continue learning the Recipe for Machine Learning, illustrated by introducing the Linear Regression model for solving Regression tasks.\n",
    "\n",
    "Our coverage of the Recipe will be rapid and shallow (we use an extremely simple example for illustration).\n",
    "\n",
    "I highly recommend reviewing and understanding\n",
    "this [Geron notebook](external/handson-ml2/02_end_to_end_machine_learning_project.ipynb)\n",
    "in order to acquire a more in-depth appreciation of the Recipe.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L3_S4_ML_Process.png\" width=\"100%\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Recipe for Machine Learning/Linear Regression** (continued)\n",
    "\n",
    "- We continue with Step 4 of the Recipe: [Train a model: Cross Validation](Recipe_Overview.ipynb#Validation-and-Cross-Validation)\n",
    "    - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "- [Iterating on the Train the model step: higher order features](Recipe_Overview.ipynb#Iterate:-Linear-Regression-with-higher-order-features)\n",
    "\n",
    "**Fitting a model: details**\n",
    "\n",
    "Recall: fitting a model (finding optimal value for the parameters) is found by minimizing a Loss function.\n",
    "\n",
    "Let's examine a typical Loss function for Regression\n",
    "- [Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "Increasing the number of parameters of a model improves in-sample fit (reduces Loss) but may compromise\n",
    "out-of-sample prediction (generalization).\n",
    "\n",
    "We examine the issues of having too many/too few parameters.\n",
    "- [When to stop iterating: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "    \n",
    "    \n",
    "**Transformations**\n",
    " - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    " \n",
    "**Deeper dives**\n",
    "- [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Task\n",
    "\n",
    "**Plan**\n",
    "- We introduce a model for the Classification task: Logistic Regression\n",
    "- How to deal with Categorical (non-numeric) variables\n",
    "    - classification target\n",
    "    - features\n",
    "\n",
    "**Classification intro**\n",
    "- [Classification: Overview](Classification_Overview.ipynb)\n",
    "- [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "**Deeper dives**\n",
    "- [Log odds](Classification_Log_Odds.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Continue with the Classification Task, following the Recipe for Machine Learning.\n",
    "\n",
    "We then examine the proper treatment of categorical variables (target or feature).\n",
    "\n",
    "Along the way, we run into a subtle difficulty: the Dummy Variable Trap.\n",
    "\n",
    "We then generalize Binary Classification into classification into more than two classes.\n",
    "\n",
    "**Classification, continued**\n",
    "\n",
    "We begin with review the Exploratory Data Analysis step that we introduced last week.\n",
    "\n",
    "- [Classification and Categorical Variables: Visualize the data](Classification_Notebook_Overview.ipynb#Visualize-Data-to-gain-insights)\n",
    "\n",
    "**Categorical variables** (contained as subsections of Classification and Categorical Variables)\n",
    "\n",
    "- [Classification and Categorical Variables: Categorical Variables](Classification_Notebook_Overview.ipynb#Categorical-variables)\n",
    "    - [Categorical variables, One Hot Encoding (OHE)](Categorical_Variables.ipynb)\n",
    "    \n",
    "**Multinomial Classification**\n",
    "\n",
    "- [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "\n",
    "**Classification and Categorical variables wrapup**\n",
    "\n",
    "- [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "- [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "- [Using pipelines to avoid cheating in cross validation](Prepare_data_Overview.ipynb#Using-pipelines-to-avoid-cheating-in-cross-validation)\n",
    "- [OHE issue: Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 4\n",
    "\n",
    "Good news\n",
    "- You now know two main tasks in Supervised Learning\n",
    "    - Regression, Classification\n",
    "- You now know how to use virtually every model in `sklearn`\n",
    "    - Consistent API\n",
    "        - `fit`, `transform`, `predict`\n",
    "- You survived the \"sprint\" to get you up and running with ML\n",
    "- You know the *mechanical process* to implement transformations: Pipelines\n",
    "\n",
    "Time to re-visit, in more depth, several important topics\n",
    "\n",
    "**Error Analysis**\n",
    "- We explain Error Analysis for the Classification Task, with a detailed example\n",
    "- How Training Loss can be improved\n",
    "\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [linked notebook](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "    - [Worked example](Error_Analysis_MNIST.ipynb)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    "\n",
    "\n",
    "**Imbalanced data**\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "**Transformations: the \"why\"**\n",
    "\n",
    "Part of becoming a better Data Scientist is transforming raw features into more useful synthetic features.\n",
    "\n",
    "In an earlier week, we presented the [mechanics](Prepare_data_Overview.ipynb) (how to use `sklearn` to implement transformation Pipelines) of Transformations.  This week, we focus\n",
    "on the necessity (the \"why\"): transforming raw data into something that tells a story\n",
    "\n",
    "- [Becoming a successful Data Scientist](Becoming_a_successful_Data_Scientist.ipynb)\n",
    "- [Transformations: overview](Transformations_Overview.ipynb)\n",
    "    - linked notebooks:\n",
    "        - [Transformations: adding a missing feature](Transformations_Missing_Features.ipynb)\n",
    "        \n",
    "        - **Deferred to following week**:\n",
    "            - [Transformations: scaling](Transformations_Scaling.ipynb)\n",
    "            - [Transformations: normalization](Transformations_Normalization.ipynb)\n",
    "            - [Other Transformations](Transformations_Other.ipynb)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5\n",
    "\n",
    "**Transformations: the \"why\" (continued)**\n",
    "\n",
    "- [Transformations: overview](Transformations_Overview.ipynb#Scaling)\n",
    "    - linked notebooks:\n",
    "        - [Transformations: scaling](Transformations_Scaling.ipynb)\n",
    "        - [Transformations: normalization](Transformations_Normalization.ipynb)\n",
    "        - [Other Transformations](Transformations_Other.ipynb)\n",
    "        \n",
    "        \n",
    "**Loss functions: mathematical basis**\n",
    "\n",
    "Where do the Loss functions of Classical Machine Learning come from ?  We take a brief mathematical\n",
    "detour into Loss functions.\n",
    "\n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "- [Loss functions: the math](Loss_functions.ipynb)\n",
    "    - Maximum likelihood\n",
    "    - Preview: custom loss functions and Deep Learning\n",
    "\n",
    "## More models for classification\n",
    "\n",
    "**Plan**\n",
    "- More models: Decision Trees, Naive Bayes\n",
    "    - Different flavor: more procedural, less mathematical\n",
    "    - Decision Trees: a model with *non-linear* boundaries\n",
    "- Ensembles\n",
    "    - Bagging and Boosting\n",
    "    - Random Forests\n",
    "\n",
    "**Decision Trees, Ensembles**\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n",
    "\n",
    "**Naive Bayes**\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6\n",
    "\n",
    "## More models for classification\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Continue with more models for classification.\n",
    "\n",
    "We begin by reviewing (and clarifying) Ensembling\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)\n",
    "\n",
    "\n",
    "**Naive Bayes** (continued)\n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "**Support Vector Classifiers**\n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)  \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "\n",
    "Just a reminder: we *already* know how to fit all the models we will introduce\n",
    "\n",
    "        for name, clf in { \"Logistic\": logistic_clf,\n",
    "                           \"SVM\": svm_clf,\n",
    "                           \"Random Forest\": forest_clf\n",
    "                         }.items():\n",
    "\n",
    "            # Combine the transformation pipeline with a final classification step\n",
    "            model_pipeline = Pipeline(steps=[ (\"transform\", preprocess_pipeline),\n",
    "                                              (\"classify\", clf)\n",
    "                                            ]\n",
    "                                     )\n",
    "\n",
    "            # Cross validation on the combined pipeline\n",
    "            scores = cross_val_score(model_pipeline, train_data, y_train, cv=10)\n",
    "            print(\"Model: {m:s} avg cross val score={s:3.2f}\\n\".format(m=name, s=scores.mean()) )\n",
    "\n",
    "            # Fit the model using all training data.\n",
    "            #  cross_val_score does not run the fit on the complete data.\n",
    "            #  It runs a number of fits, each fit being on the data with one fold held out for validation.\n",
    "            _= model_pipeline.fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised Learnig\n",
    "\n",
    "**Unsupervised Learning: PCA**\n",
    "- [Unsupervised Learning: Overview](Unsupervised_Overview.ipynb)\n",
    "- [PCA Notebook Overview](Unsupervised_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Unsupervised.ipynb)\n",
    "- [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 7 \n",
    "\n",
    "## Unsupervised Learning (continued)\n",
    "\n",
    "**Unsupervised Learning: PCA** (continued)\n",
    "- [PCA (continued)](Unsupervised.ipynb#PCA-in-sklearn)\n",
    "    - [PCA in Finance](PCA_Yield_Curve_Intro.ipynb)\n",
    "\n",
    "\n",
    "## Classical ML Coda: deeper dives\n",
    "\n",
    "**Deeper Dives**\n",
    "- [Linear Regression in more depth](Linear_Regression_fitting.ipynb)\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "- [Missing data: clever ways to impute values](Missing_Data.ipynb)\n",
    "- [Feature importance](Feature_Importance.ipynb)\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
    "\n",
    "## Bridge between Classical ML and Deep Learning\n",
    "\n",
    "**Gradient Descent** \n",
    "\n",
    "Machine Learning is based on minimization of a Loss Function.  Gradient Descent is one algorithm\n",
    "to achieve that.\n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "\n",
    "**Recommender Systems (Pseudo SVD)**\n",
    "\n",
    "How does Amazon/Netflix/etc. recommend products/films to us ?  We describe a method similar to SVD\n",
    "but that is solved using Gradient Descent.\n",
    "\n",
    "This theme of creating a custom Loss Functions and minimizing it via Gradient Descent is a recurring\n",
    "theme in the upcoming Deep Learning second half of the course.\n",
    "\n",
    "- [Recommender Systems](Recommender_Systems.ipynb)\n",
    "- [Preview: Some cool Loss functions](Loss_functions.ipynb#Loss-functions-for-Deep-Learning:-Preview)\n",
    "\n",
    "**Deeper Dives**\n",
    "\n",
    "- [Other matrix factorization methods](Unsupervised_Other_Factorizations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Deep Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "# Week 1 Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "**Gradient descent (continued)**\n",
    "\n",
    "[Gradient Descent: Improvements](Gradient_Descent.ipynb#Improvements-to-Gradient-Descent)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Plan**\n",
    "\n",
    "Deep Learning/Neural networks\n",
    "\n",
    "- [Set up your Tensorflow environment](Tensorflow_setup.ipynb)\n",
    "- [Neural Networks Overview](Neural_Networks_Overview.ipynb)\n",
    "\n",
    "\n",
    "**Neural network: practical**\n",
    "- Coding Neural Networks: Tensorflow, Keras\n",
    "    - [Intro to Keras](Tensorflow_Keras.ipynb)\n",
    "\n",
    "    - **Note**\n",
    "        - If you have problems using the `plot_model` function in Keras on your local machine: see [here](Setup_ML_Environment_NYU.ipynb#Tools-for-visualization-of-graphs-(optional)) for a fix.\n",
    "\n",
    "\n",
    "- Practical Colab\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (neural_net_helper.py) --->\n",
    "<!--- The Colab notebook imports some modules; make sure they are in the repo --->\n",
    "<!--- #include (Colab_practical.ipynb)) --->\n",
    "   - **Colab**: [Practical Colab Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2024/blob/master/Colab_practical.ipynb)\n",
    "   \n",
    "**Practical advice**\n",
    "\n",
    "- Karpathy: [Recipe for training Neural Nets](Karpathy_Recipe_for_training_NN.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Plan**\n",
    "\n",
    "The topics introduced in the Neural Networks Overview are now covered more in-depth.\n",
    "- Where do Neural Networks get their power from ?\n",
    "- How exactly do we compute the gradients ?\n",
    "- How does a special language/library facilitate automatic computation of the gradients ?\n",
    "\n",
    "\n",
    "**Neural network theory**\n",
    "- [A neural network is a Universal Function Approximator](Universal_Function_Approximator.ipynb)\n",
    "\n",
    "**Training Neural Networks (introduction)**\n",
    "- [Training Neural Networks - Back propagation](Training_Neural_Network_Backprop.ipynb)\n",
    "\n",
    "**How to compute gradients automatically**\n",
    "- [Why TensorFlow ?: Gradients made easy](Training_Neural_Network_Operation_Forward_and_Backward_Pass.ipynb)\n",
    "\n",
    "   \n",
    "**Deeper Dives**\n",
    "<!--- #include (Raw_TensorFlow.ipynb)) --->\n",
    "- [Keras, from past to present](Tensorflow_Keras_Archaeology.ipynb)\n",
    "- [History/Computation Graphs: Tensorflow version 1](DNN_TensorFlow_Using_TF_version_1.ipynb)\n",
    "- [Raw_TensorFlow example Notebook from github](https://colab.research.google.com/github/kenperry-public/ML_Spring_2024/blob/master/Raw_TensorFlow.ipynb) (**Colab**)\n",
    "- [Computation Graphs](Computation_Graphs.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 2  Convolutional Neural Networks\n",
    "\n",
    "\n",
    "**Convolutional Neural Networks (CNN)**\n",
    "\n",
    "- The Convolutional Neural Network layer type\n",
    "\n",
    "- [Introduction to CNN](Intro_to_CNN.ipynb)\n",
    "- [CNN: multiple input/output features](CNN_Overview.ipynb)\n",
    "    - [CNN pictorial](CNN_pictorial.ipynb)\n",
    "- [CNN: Space and Time](CNN_Space_and_Time.ipynb)\n",
    "    - [CNN example from github](https://colab.research.google.com/github/kenperry-public/ML_Fall_2023/blob/master/CNN_demo.ipynb) (**Colab**) \n",
    "    - [CNN example from github](CNN_demo.ipynb) (**local machine**) \n",
    "\n",
    "\n",
    "**Deeper dives**\n",
    "- [Convolution as Matrix Multiplication](CNN_Convolution_as_Matrix_Multiplication.ipynb)\n",
    "\n",
    "**Training Neural Networks: the fine details**\n",
    "- [The dynamics of training](Training_Neural_Networks_Overview.ipynb)\n",
    "    - Effects of changing: activation functions; weight initialization\n",
    "    - initialization and scaling\n",
    "    - dropout\n",
    "    - learning rate schedules\n",
    "    - vanishing/exploding gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "- Assignment notebook: [Using Machine Learning for Hedging](assignments/Regression%20task/Using_Machine_Learning_for_Hedging.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Regression task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "- Assignment notebook: [Ships in satellite images](assignments/Classification%20task/Ships_in_satellite_images.ipynb#)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Classification task\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Midterm Project: Bankruptcy One Year Ahead\n",
    "- Assignment notebook [Bankruptcy One Year Ahead](assignments/bankruptcy_one_yr/Bankruptcy_oya.ipynb)\n",
    "- Data\n",
    "    - There is an archive file containing the data\n",
    "    - You can find it\n",
    "        - Under the course page: Content --> Data --> Assignments --> Bankruptcy One Year Ahead\n",
    "        - You won't be able to view the file in the browser, but you **will** be able to Download it\n",
    "    - You should unzip this archive into the *the same directory* as the assignment notebook\n",
    "    - The end result is that the directory should contain\n",
    "        - The assignment notebook and a helper file\n",
    "        - A directory named `Data`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras practice\n",
    "- Assignment notebook [Ships in satellite images: Neural Network](assignments/keras_intro/Ships_in_satellite_images_P1.ipynb)\n",
    "- Data (same as for the Classification assignment)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
