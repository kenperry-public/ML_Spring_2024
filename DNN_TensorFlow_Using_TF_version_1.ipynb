{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical perspective: Tensorflow version < 2\n",
    "\n",
    "We will be using Tensorflow version 2 in this course.\n",
    "- It has integrated the higher-level Keras API\n",
    "- It uses \"eager execution\"\n",
    "\n",
    "This notebook shows you\n",
    "- The lower level non-Keras API\n",
    "- Non-eager execution\n",
    "\n",
    "The purpose\n",
    "- It is interesting from an historical perspective\n",
    "- Might give you an appreciation of Computation Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IDpTj3R2War"
   },
   "source": [
    "# Derived from Geron 11_deep_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0VTrSvDJlis"
   },
   "source": [
    "We will provide a quick introduction into programming with TensorFlow.\n",
    "\n",
    "We revisit our old friend, MNIST digit classification and provide two solutions\n",
    "- the first using \"raw\", low-level TensorFlow\n",
    "- the second using the high-level Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEkMfar38_Q7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kjp/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "USE_TF_VERSION=1\n",
    "\n",
    "if USE_TF_VERSION < 2:\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "else:\n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyT0csrdV2X7"
   },
   "source": [
    "# Raw TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYdRJiVS2k7c"
   },
   "source": [
    "# TensorFlow.layers\n",
    "\n",
    "We will build an MNIST classifier using TensorFlow.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0RoR15U3N2_"
   },
   "source": [
    "## Get the MNIST dataset\n",
    "- data presplit into training and test sets\n",
    "  - flatten the images from 2 dimensional to 1 dimensional (makes it easier to feed into first layer)\n",
    "  - create validation set from part of training\n",
    "- \"normalize\" the inputs: change pixel range from [0,255] to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3Cdg32u3Lon"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Determine \n",
    "# - the dimensions of the input by examining the first training example\n",
    "# - the dimensions of the output (number of classes) by examinimg the targets\n",
    "input_size = np.prod(X_train[0].shape)\n",
    "output_size = np.unique(y_train).shape[0]\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = X_train[0].shape[0:2]\n",
    "\n",
    "valid_size = X_train.shape[0] // 10\n",
    "\n",
    "# Flatten the data to one dimension and normalize to range [0,1]\n",
    "X_train = X_train.astype(np.float32).reshape(-1, input_size) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, input_size) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:valid_size], X_train[valid_size:]\n",
    "y_valid, y_train = y_train[:valid_size], y_train[valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "US5Qs7m7AA66",
    "outputId": "361f3d47-7aca-49e5-bb13-52e75bb5b947"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(n_hidden_1, n_hidden_2) = (100, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IV00RzrR23aF"
   },
   "outputs": [],
   "source": [
    "# Placeholders for input X, target y\n",
    "#  The first dimension (None) is for the batch size\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, input_size), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EL4y6kMp825O"
   },
   "source": [
    "## Create function to return mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKWxF7CF87TM"
   },
   "outputs": [],
   "source": [
    "def next_batch(X, y, batch_size, shuffle=True):\n",
    "  \"\"\"\n",
    "  Generator to return batches from X and y\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  X: ndarray\n",
    "  y: ndarray.  The first dimension of X and y must be the same\n",
    "  batch_size: Int.  The size of the slice (of X and y) to return in each batch\n",
    "  shutffle: Boolean.  Sample X, y in random order if True\n",
    "  \n",
    "  Yields\n",
    "  ------\n",
    "  X_batch, y_batch: a 2-tuple of ndarrays, \n",
    "  - where X_batch is a slice (of size at most batch_size) of X\n",
    "  - where y_batch is a slice of y (same first dimension as X_batch)\n",
    "  \n",
    "  If first dimension of X is not evenly divisible by batch size, the final batch will \n",
    "  be of size smaller than batch_size\n",
    "  \"\"\"\n",
    "  \n",
    "  # Randomize the indices\n",
    "  if shuffle:\n",
    "    idx = np.random.permutation(len(X))\n",
    "  else:\n",
    "    idx = np.arange( len(X) )\n",
    "\n",
    "  # Return a batch of size (at most) batch_size, \n",
    "  # starting at idx[next_start] \n",
    "  next_start = 0\n",
    "\n",
    "  n_batches = len(X) // batch_size\n",
    "  \n",
    "  while next_start < len(X):\n",
    "    # Get a batch of indices from idx, starting a idx[next_start] and ending at idx[next_end]\n",
    "    next_end   = min(next_start + batch_size, len(X))\n",
    "    X_batch, y_batch = X[ idx[next_start:next_end] ], y[ idx[next_start:next_end] ]\n",
    "\n",
    "    # Advance next_start to start of next batch\n",
    "    next_start = next_start + batch_size\n",
    "\n",
    "    # Return a batch\n",
    "    yield X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7lOsZRd_py6"
   },
   "source": [
    "## Build the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n_hidden_1, n_hidden_2) = (100, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_RJw1dCT_WO"
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6f0gWZm_sTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-7a8e8474ba41>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/kjp/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "# Placeholders for input X, target y\n",
    "#  The first dimension (None) is for the batch size\n",
    "X = tf.placeholder(tf.float32, shape=(None, input_size), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden_1, activation=\"relu\", name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden_2, activation=\"relu\", name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, output_size, name=\"outputs_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcKWUh7bAzTw"
   },
   "source": [
    "## Create a loss node\n",
    "- Use cross entropy as loss \n",
    "  - we are comparing the probability vector computed by the graph (logits) with the target probability vector (y)\n",
    "  \n",
    "Ordinarily we would need to\n",
    "- convert the scores (logits) vector to a probability vector  by a *softmax* activation on the \"outputs\" layer\n",
    "- convert the target to a one-hot vector (length equal to number of target classes, which is also length of probability vector)\n",
    "- compare the two vectors with cross_entropy\n",
    "\n",
    "TensorFlow provides a very convenient method `sparse_softmax_cross_entropy_with_logits` that does all the work for us !\n",
    "- applies `softmax` to the scores (logits)\n",
    "- converts integer targets (in range [0, number of classes]) into one-hot vectors (with length equal to number of classes)\n",
    "- does the cross entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dV7TYru5A-FX"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "  # xentropy is a tensor whose first dimension is the batch size\n",
    "  xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "  \n",
    "  # Find the loss across the examples in the batch by summing individual example losses\n",
    "  loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXUDMB6NERjz"
   },
   "source": [
    "## Create a node to compute accuracy \n",
    "-  for each example, compares the element in the logit vector with the highest score (i.e., index of our prediction) to the target\n",
    "- sums up the number of examples with matching max logit and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p96JUARmEQhz"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "  correct = tf.nn.in_top_k(logits, y, 1)\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4fn7An1D2Px"
   },
   "source": [
    "## Create the training operations\n",
    "- Training operation is an optimizer step that minimizes the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEs3eok8D8Vz"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxYzMnDWG9d5"
   },
   "source": [
    "## Create an initialization node to initialize global variables (i.e., the weights that the optimizer will solve for)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcG0YcU2HEDm"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9TW48GdEwlZ"
   },
   "source": [
    "## Run  the training loop\n",
    "- Run for multiple \"epochs\"; an epoch is an entire pass through the training data set\n",
    "- For each epoch, divide the training set into mini-batches\n",
    "  - For each mini-batch\n",
    "    - run the \"training operation\" (i.e, the optimizer)\n",
    "    - every few epochs\n",
    "      - compute the accuracy (by evaluating the graph node that computes accuracy) on the training and validation set\n",
    "      \n",
    "In general, we usually continue training\n",
    "- as long as the validation loss continues to decrease across epochs\n",
    "- as long as the validation loss is greater than the training loss\n",
    "  - if the training loss is much lower than the validation (out of sample) loss, we may be overfitting to the training data\n",
    "  - **Note** that we have stated these conditions in terms of decreasing validation loss, rather than increasing validation accuracy\n",
    "    - **Question**: *Why should we prefer \"loss\" to \"accuracy\" ?*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwZ9NvFCEzcJ"
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "modelName = \"mnist_first\"\n",
    "\n",
    "save_path = os.path.join(\".\", modelName + \".ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "t5aYMTwk_fry",
    "outputId": "8d2bb4cd-8b96-4f10-a746-dd920fc6ca5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs\n",
      "Epoch 0 training batch accuracy 92.00%, validation set accuracy 88.78%:\n",
      "Epoch 5 training batch accuracy 86.00%, validation set accuracy 94.50%:\n"
     ]
    }
   ],
   "source": [
    "print(\"Training for {e:d} epochs\".format(e=n_epochs))\n",
    "\n",
    "# Create a session and evaluate the nodes within it\n",
    "with tf.Session() as sess:\n",
    "  # Run the initialization step\n",
    "  init.run()\n",
    "  \n",
    "  # This is our main training loop\n",
    "  # - run for multiple epochs\n",
    "  # - in each epoch, process the entire training data in mini-batches\n",
    "  for epoch in range(n_epochs):\n",
    "    # Process each of the mini-batches, evaluating the training operation on each\n",
    "    for X_batch, y_batch in next_batch(X_train, y_train, batch_size, shuffle=True):\n",
    "      sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "    # Measure the training and validation accuracy every few epochs \n",
    "    if epoch % 5 == 0:\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(\"Epoch {e:d} training batch accuracy {ta:3.2f}%, validation set accuracy {va:3.2f}%:\".format(e=epoch, ta=100*acc_batch, va=100*acc_valid) )\n",
    "\n",
    "  # Save the session so we can pick up again      \n",
    "  save_path = saver.save(sess, save_path)\n",
    "  \n",
    "  print(\"Trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMp3oR3W04MB"
   },
   "source": [
    "Now that the model is trained (and saved) we can feed in test data in order to perform predictions\n",
    "\n",
    "Note that:\n",
    "- The graph must always be evaluated in a session\n",
    "- A new session is completely uninitialized\n",
    "  - the trained weights are *not* available\n",
    "- We can restore the state of a previous session, in order to obtain access to the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "wecpagmYx8RU",
    "outputId": "9073603c-92d9-452c-9005-7d6c40cfc3e8"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  # Restore the model, do NOT re-initialize the variables with the \"init\" node\n",
    "  saver.restore(sess, save_path)\n",
    "  \n",
    "  # We can now evaluate any of the model's nodes, using the trained weights\n",
    "  # Perform prediction using the test set\n",
    "  # Recall: the logits for each example is a vector of length, number of classes\n",
    "  # To convert one vector to a prediction: find the index of the largest logit\n",
    "  logits_test = logits.eval(feed_dict={X: X_test})\n",
    "  print(\"Test logits shape: \", logits_test.shape)\n",
    "  predictions_test = np.argmax(logits_test, axis=1)\n",
    "  \n",
    "  # Show some of the predictins\n",
    "  num_to_show = 10\n",
    "  print(\"Test predictions: \\t\",  predictions_test[:num_to_show])\n",
    "  \n",
    "  print(\"Test correct ?:\\t \",    (predictions_test == y_test)[:num_to_show])\n",
    "  \n",
    "  # What is the overall accuracy ?\n",
    "  acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "  print(\"Test accuracy {a:3.2f}\".format(a=100*acc_test))\n",
    "  \n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DNN_TensorFlow_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
