{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( #1 \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Impediments to learning\n",
    "\n",
    "The updating of weights, used by Gradient Descent to minimize the loss, can be inhibited in less-than-obvious manners.\n",
    "\n",
    "In this module, we explore these impediments.\n",
    "\n",
    "This will motivate the creation of a new class of Layer-types: Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simply stated: the power of Neural Networks derives largely from\n",
    "- the non-linear activation functions \n",
    "- the simple weight updating of Gradient descent\n",
    "\n",
    "But these same non-linear activation functions that provide power sometimes interact badly\n",
    "with Gradient updates.\n",
    "\n",
    "Let us examine the plots of the derivatives of the common Activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, _ = nnh.plot_activations( np.arange(-5,5, 0.1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The derivatives of each of these functions has a portion of their domain yielding near zero derivatives.\n",
    "\n",
    "When the unit that feeds its activation to this Activation function has a value in this domain\n",
    "- the derivative is zero\n",
    "- the weight associated with the unit producing this activation will receive a **zero** update from Gradient Descent\n",
    "    - the unit won't learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, there are also portions of the domain with near-linear response.\n",
    "\n",
    "The composition of linear operations is linear, so a deep network without some active non-linearity\n",
    "loses all its power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"trick\" to successful learning is to ensure that\n",
    "- the distribution of pre-activations \n",
    "- are neither **concentrated** in parts of the domain\n",
    "    - with zero derivative\n",
    "    - near-linear response\n",
    "    \n",
    "Learning works best when the pre-activation distribution is spread-out rather than \"peaky\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This module will highlight how a Neural Network may\n",
    "- wind up in one of the undesirable situations\n",
    "- the mechanisms that have been devised to avoid them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proper scaling of inputs\n",
    "\n",
    "We briefly explore the statistical properties of the outputs of a layer.\n",
    "- We show how some of these properties can inhibit learning (weight update)\n",
    "- Will motivate the Normalization Layer-type, which will maintain good properties of layer outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importance of zero centered inputs (for each layer)\n",
    "[Efficient Backprop paper, LeCunn98](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "**Zero centered** means average (over the training set) value of each feature of examples is mean $0$.\n",
    "\n",
    "Gradient descent updates each element of a layer $\\ll$'s weights $\\W_\\llp$ by\n",
    "the per-example losses \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss^\\ip }{\\partial W_\\llp} & = & \\frac{\\partial \\loss^\\ip}{\\partial \\y_\\llp^\\ip} \\frac{\\partial \\y_\\llp^\\ip}{\\partial \\W_\\llp} \n",
    "\\end{array}\n",
    "$$\n",
    "summed over examples $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look into the per example loss in more detail.\n",
    "\n",
    "\n",
    "Since $\\W_\\llp$ is a vector, the derivative wrt $\\W_\\llp$ is a vector of derivatives:\n",
    "$$\n",
    "\\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_\\llp } =\n",
    "\\begin{pmatrix} \n",
    "\\ldots , \\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_{\\llp,} }, \\ldots, \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Examining the $j^{th}$ element of the derivative vector:\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_{\\llp,j} } & = & \\frac{ \\partial{ a_\\llp ( \\y_{(\\ll-1)}\\cdot \\W_\\llp ) } } { \\partial \\W_{\\llp,j}} &  \\text{ when layer } \\ll \\text{ is Dense since }  y_\\llp = a_\\llp ( \\y_{(\\ll-1)} \\cdot \\W_\\llp ) \\; \\\\\n",
    "& = &  \\frac{ \\partial{ a_\\llp ( \\y_{(\\ll-1)}\\cdot \\W_\\llp )} } { \\partial (\\y_{(\\ll-1)}\\cdot \\W_{\\llp})}\n",
    "       \\frac{ \\partial{(\\y_{(\\ll-1)}\\cdot \\W_\\llp)} }{ \\W_{\\llp,j} } & \\text{Chain rule} \\\\\n",
    "& =& a'_\\llp \\y_{(\\ll-1),j}^\\ip & \\text{ since }  \\y_{(\\ll-1)}\\cdot \\W_\\llp = \\sum_j { ( \\y_{(\\ll-1),j} * \\W_{\\llp,j} ) }\\\\\n",
    " & & & \\text{where } a' = \\frac{ \\partial{ a_\\llp ( \\y_{(\\ll-1)}\\cdot \\W_{\\llp}) } } { \\partial (\\y_{(\\ll-1)}\\cdot \\W_{\\llp})}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is for $\\loss^\\ip$, the per-example loss for example $i$.\n",
    "\n",
    "The (total) Loss $\\loss$ is averaged across all $m'$ examples in the mini-batch\n",
    "\n",
    "So the derivative of the Loss (with respect to the $j^{th}$ weight) $\\frac{\\partial \\loss }{\\partial W_{\\llp, j}}$ will have the term\n",
    "$$\n",
    "\\sum_{i=0}^{m'} {  \\y_{(\\ll-1),j}^\\ip  }\n",
    "$$\n",
    "\n",
    "Thus, the update to $\\W_{\\llp, j}$ will be proportional to the average (across the $m'$ examples) of the $j^{th}$ input to layer $\\ll$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be concrete, let's focus on layer $1$, where \n",
    "$$\\y_{(\\ll -1),j} = \\x_j$$\n",
    "so that\n",
    "$$\n",
    "\\sum_{i=0}^m {  \\y_{(\\ll-1),j}^\\ip  } = \\bar\\x_j\n",
    "$$\n",
    "i.e., the average (across examples) value of input feature $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the particular case that the average $\\bar{\\x}_j$ of *every* feature $j$ has the same sign:\n",
    "- updates in all weight dimensions will have the same sign (the sign of $a'$)\n",
    "- Example: two dimensions.  The weight space is $\\W_{\\llp,0} \\times \\W_{\\llp,1}$\n",
    "    - We can navigate the the loss surface by moving in the weight space **north-east** or **south-west** only ! \n",
    "        - to move south-east: we need to move in a \"zig-zag\" direction\n",
    "    - very inefficient way to explore the weight space\n",
    "<br>\n",
    "<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>RNN many to one API</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/zig_zag_path.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although we have illustrated this issue using layer $1$, the issue applies to each layer.\n",
    "\n",
    "In fact, the issue may be more likely in deeper layers\n",
    "- when the activation of layer $(\\ll-1)$ is *not* zero-centered, e.g., the ReLU and sigmoid\n",
    "\n",
    "This will motivate the creation of a new layer type whose purpose will be to keep the inputs to successive layers zero-centered.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Although we zero center the $m$ examples in the training set, the $m' \\lt m$ examples in any mini-batch will not necessarily be zero mean in all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importance of identical variance across features (weight initialization)\n",
    "\n",
    "The same argument we made for zero-centering a feature can be extended to it's variance:\n",
    "- the variance of feature $j$ over all training examples $i$ is the variance of $\\y_{(\\ll-1),j}$\n",
    "\n",
    "If the variance of features $j$ and $j'$ are different, their updates will happen at different rates.\n",
    "\n",
    "We will examine this in greater depth during our discussion of weight initialization.\n",
    "\n",
    "For now: it is desirable that the input to *each* layer have it's features somewhat normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initialization\n",
    "\n",
    "Training is all about discovering good weights.\n",
    "\n",
    "As prosaic as it sounds: how do we *initialize* the weights before training ?\n",
    "Does it matter ?\n",
    "\n",
    "It turns out that the choice of initial weights does matter.\n",
    "\n",
    "We will explain why in this module.\n",
    "- There is an excellent [visualization tool](https://www.deeplearning.ai/ai-notes/initialization/index.html#playground) to illustrate these concepts\n",
    "    - Play around with different initialization\n",
    "    - Visualize the weights, gradients\n",
    "    \n",
    "Let's start with some *bad* choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bad choices\n",
    "\n",
    "### Too big/small\n",
    "\n",
    "Layers usually consist of linear operations (e.g., matrix multiplication and addition of bias)\n",
    "followed by a non-linear activation.\n",
    "\n",
    "The range of many activation functions includes large regions where the derivatives are near zero,\n",
    "usually corresponding to very large/small activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient Descent updates weights using the gradients.\n",
    "\n",
    "Obviously, if the gradients are all near-0, learning cannot occur.\n",
    "\n",
    "So one bad choice is any set of weights that tends to push activations to regions of the non-linear\n",
    "activation with zero gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dead Activation functions\n",
    "\n",
    "This is a special case of a too big/small initialization of the **bias** of a unit.\n",
    "\n",
    "Recall that a layer's pre-activation value is computed as\n",
    "- dot product of (some or all) layer inputs and weights\n",
    "- plus the bias (if not already incorporated in the weight vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the layer learns a very large negative bias\n",
    "- the pre-activation value will likely be a large negative value\n",
    "- placing the pre-activation value in a region of the Activation Function with zero gradients\n",
    "- resulting in no update to the bias\n",
    "\n",
    "Hence, the negative bias with no updates will persist across training steps.\n",
    "- the unit will not contribute to distinguishing between input examples\n",
    "\n",
    "We call a unit with constant (commonly $0$, e.g., ReLU) output\n",
    "- *saturated*\n",
    "- *dead* activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Identical weights\n",
    "\n",
    "Consider layer $\\ll$ with $n_\\ll$ units (neurons) implementing identical operations (e.g. FC + ReLu).\n",
    "\n",
    "Let  $\\W_{\\llp, k}$ denote the weights of unit $k$.\n",
    "\n",
    "Suppose we initialized the weights (and biases) of all units to the *same* vector.\n",
    "$$\n",
    "\\W_{\\llp, k} = \\w_\\llp, \\; 1 \\le k \\le n_\\ll\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider two neuron $j, j'$ in the same layer $\\ll$\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{\\llp, j}  & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\y_{\\llp, j'} & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Both neuron will compute the same activation\n",
    "- Both neurons will have the same gradient\n",
    "- Both neurons will have the same weight update\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the weights in layer $i$ will start off identical and will remain identical due to identical updates!\n",
    "\n",
    "Neurons/units $j$ and $j'$ will never be able to differentiate and come to recognize *different* features.\n",
    "\n",
    "This negates the advantage of multiple units in a layer.\n",
    "\n",
    "Many approaches use some for of random initialization to break the symmetry we just described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This explains the chart we previously saw (training when all weights initialized to zero)\n",
    "- All weights identical, so no way to break symmetry\n",
    "- Zero weights lead to zero activation\n",
    "\n",
    "<img src=\"images/tnn_loss_and_acc_zero_init.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maintaining constant variance throughout the layers\n",
    "\n",
    "Suppose the variance of the features of the input vector for layer $\\ll$ (i.e., $\\y_{(\\ll-1)}$)\n",
    "- is identical and equal to $\\text{Var}$ ($\\y_{(\\ll-1)}$)\n",
    "\n",
    "Once it passes through layer $\\ll$, what is the output variance $\\text{Var}$ ($y_\\llp)$ ?\n",
    "- which is the variance of the input of layer $\\ll+1$.\n",
    "\n",
    "We can illustrate with an experiment\n",
    "- a network consisting of a sequence of identically sized Fully Connected layers\n",
    "- plotting the distribution of layer outputs by layer\n",
    "- through a single epoch: no weight updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we initialize the layer weights with a relatively large variance ($\\sigma = 1$)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Distribution of layer outputs, by layer (shallow to deep)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><center>Random Normal weight initialization</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/tnn_activations_shift_large_var.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The large variance forces the pre-activations toward extreme value.  Hence\n",
    "- the bimodal peaks for the tanh activation\n",
    "- the large peak of zero values for the ReLU activation\n",
    "\n",
    "Note that the variance is increasing as we get deeper (top to bottom direction of graphs) into the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a very small variance when initializing weights ($\\sigma = 0.01$)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Distribution of layer outputs, by layer (shallow to deep)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><center>Random Normal weight initialization</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/tnn_activations_shift_small_var.png\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The small weight variance causes the pre-activations to tightly cluster\n",
    "- potentially putting us in the linear regions\n",
    "    - 0.5 for sigmoid activation\n",
    "    - 0 for tanh activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The cause\n",
    "\n",
    "Let's examine the mathematical properties of the *pre-activation distribution* of a\n",
    "Fully Connected layer $\\ll$.\n",
    "\n",
    "That is, the distribution of the dot product \n",
    "$$\n",
    "\\y_{(\\ll-1)} \\cdot W_\\llp\n",
    "$$\n",
    "for layer $\\ll$ computing\n",
    "\n",
    "$$f_\\llp(\\y_{(\\ll-1)}, W_\\llp) = \\y_{(\\ll-1)} \\cdot W_\\llp$$\n",
    "\n",
    "Recall that layer $(\\ll-1)$ has $n_{(\\ll-1)}$ outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider one product term in the dot product $\\y_{(\\ll-1),j} * \\W_{\\llp,j}$\n",
    "\n",
    "We will denote the variance \n",
    "- of $\\y_{(\\ll-1),j}$ as $\\text{Var}(Y)$\n",
    "- of $\\W_{\\llp,j}$ as $\\text{Var}(W)$\n",
    "\n",
    "and assume both variances are constant across features $j$.\n",
    "\n",
    "Note: we are making **no** assumption as to the form (e.g., Normal or Uniform) of the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *variance* of a product of random variables $Y, W$ \n",
    "[is](https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables)\n",
    "\n",
    "$$\n",
    "\\text{Var}(Y * W) = \\mathbb{E}(Y)^2 \\text{Var}(W) + \\mathbb{E}(W)^2 \\text{Var}(Y) + \\text{Var}(Y)\\text{Var}(W)\n",
    "$$\n",
    "\n",
    "For simplicity: let's assume $\\mathbb{E}(Y)$ and $\\mathbb{E}(W)$ are both zero.\n",
    "\n",
    "Then \n",
    "$$\n",
    "\\text{Var}(Y * W) = \\text{Var}(Y)\\text{Var}(W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem is that the dot product has *many* product terms\n",
    "- $n_{(\\ll-1)}$ for the input to a Fully Connected layer $\\ll$\n",
    "\n",
    "Hence, the variance of each output feature of Fully Connected layer $\\ll$ is\n",
    "$$\n",
    "n_{(\\ll-1)} * \\text{Var}(W) * \\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "That is:\n",
    "- the variance of each feature of the output pre-activation\n",
    "- is $n_{(\\ll-1)} * \\text{Var}(W)$ greater than the layer's input variance $\\text{Var}(Y)$\n",
    "\n",
    "Thus, the variance of the pre-activation distribution increases as we proceed deeper into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As the variance of the pre-activation gets larger\n",
    "- we are more likely to be in one of the extremes of the domain of the Activation function\n",
    "- where derivatives are often near-zero\n",
    "\n",
    "Hence, we wind up in an unfavorable region of the Activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Glorot initialization\n",
    "\n",
    "Fortunately, there is a simple solution to the exploding pre-activation variance\n",
    "- initialize the weights $W_\\llp$ of Fully Connected layer $\\ll$\n",
    "- to have variance $\\frac{1}{n_{(\\ll-1)}}$\n",
    "- by initializing to a unit variance distribution and scaling by \n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{n_{(\\ll-1)}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This forces the dot product to have unit variance\n",
    "\n",
    "This is the basis for *Glorot/Xavier Initialization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This only partially solves the problem as it only ensures unit variance of the **input** to the activation function.\n",
    "\n",
    "If we can make the **output** of the activation also have unit variance then\n",
    "- the input to each layer is unit variance\n",
    "- and the output (which is input to the next layer) is also unit variance\n",
    "\n",
    "Hence the output variance of each activation function remains constant as we proceed through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The [original Glorot paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) justifies this\n",
    "- By assuming either a $\\tanh$ or sigmoid activation function\n",
    "- Which are approximately linear in the active region.\n",
    "- So the **output** of the activation function is equal to the input in this region\n",
    "- And is therefore unit variance as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus far, we have achieved unit variance during the forward pass.\n",
    "\n",
    "During back propagation\n",
    "- It can be  shown that the scaling factor\n",
    "- Depends on the number of outputs $n_\\llp$ of layer $\\ll$, rather than the number of inputs $n_{(\\ll-1)}$\n",
    "    - because we move from deep to shallow in the backward pass\n",
    "    - rather than shallow to deep as n the forward pass\n",
    "- Thus, the scaling factor needs to be $\\frac{1}{\\sqrt{n_\\llp}}$ rather than $\\frac{1}{\\sqrt{n_{(\\ll-1)}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Taking the average of the two scaling factors gives a final factor of\n",
    "$\\frac{1}{\\sqrt{ \\frac{ n_{(\\ll-1)} + n_\\llp}{2} } } = \\sqrt{\\frac{2}{n_{(\\ll-1)} + n_\\llp}}\n",
    "$\n",
    "\n",
    "which is what you often see in papers using this form of initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaiming/He initialization\n",
    "\n",
    "Glorot/Xavier initialization was tailored to two particular activation functions ($\\tanh$ or sigmoid).\n",
    "\n",
    "[Kaiming et al](https://arxiv.org/pdf/1502.01852.pdf) extended the results\n",
    "to the ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ReLU activation has two distinct regions: one linear (for inputs greater than 0) and one all zero.\n",
    "\n",
    "The linear region of the activation corresponds to the assumption of the Glorot method.\n",
    "\n",
    "So if inputs to the ReLU are equally distributed around 0, this is approximately the same\n",
    "as the Glorot method with half the number of inputs.\n",
    "- that is: half of the ReLU's will be in the active region and half will be in the inactive region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Kaiming scaling factor is thus:\n",
    "$$\n",
    "\\sqrt{\\frac{2}{n_{(\\ll-1)}} }\n",
    "$$\n",
    "in order to preserve unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limits of good initialization\n",
    "\n",
    "The Glorot and Kaiming/He initializers implicitly assume\n",
    "- a sequential network: connections only between consecutive layers\n",
    "\n",
    "It has been observed that, when Residual connections are allowed, the analysis is flawed.\n",
    "\n",
    "Other techniques (such as Batch Normalization, to be introduced shortly) must be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer-wise pre-training\n",
    "\n",
    "In the early days of Deep Learning\n",
    "- Before good weight initialization techniques were discovered\n",
    "- A technique called *Layer-wise pre-training* was very popular\n",
    "\n",
    "We can motivate this technique by briefly introducing an Autoencoder network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Autoencoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/Autoencoder_vanilla.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An Autoencoder network has two parts\n",
    "- An Encoder, which takes input $\\x$ and \"encodes\" it into $\\z$\n",
    "- A Decoder, which takes the encoding $\\z$ and tries to reproduce $\\x$\n",
    "\n",
    "Each part has its own weights, which can be discovered through training, with examples\n",
    "- $\\langle \\X, \\y \\rangle = \\langle \\X, \\X \\rangle$\n",
    "\n",
    "That is: we are asking the output to be identical to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This will not be possible\n",
    "when the dimension of $\\z$ is less than the dimension of $\\x$.\n",
    "- $\\z$ is a *bottle-neck*\n",
    "\n",
    "$\\z$ becomes a *reduced-dimensionality* approximation of $\\x$.\n",
    "\n",
    "This is quite similar to discovering Principal Components.\n",
    "- We discover a small number of synthetic features $\\z$ that summarize the diversity of $\\y_{(\\ll-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What does this have to do with layer-wise initialization of weights ?\n",
    "- We construct an Autoencoder with layer $\\ll$ serving the role of the Encoder\n",
    "- We \n",
    "    - train the Autoencoder on layer $\\ll$ inputs\n",
    "    - and subsequently discard the Decoder\n",
    "\n",
    "The weights for layer $\\ll$ obtained from Autoencoder training\n",
    "- are non-random\n",
    "- meaningful in the sense that they solve the Autoencoding task\n",
    "- hopefully transfer to our original task better than random weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weights we create\n",
    "- Are not random, they meet the Autoencoder task objective\n",
    "- Perhaps non-random weights are better initializers because they discover some structure of the input\n",
    "\n",
    "Transfer Learning (the subject of another module) works in a similar manner\n",
    "- Use the weights obtained from training on a Source task\n",
    "- To use as initial weights for a second Target task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Normalization\n",
    "\n",
    "We addressed the importance of normalization of the inputs to layer $\\ll = 1$.\n",
    "\n",
    "But confining our attention to layer $1$ is not sufficient\n",
    "- The same argument for normalization of layer inputs applies to *all* layers $\\ll > 0$.\n",
    "- Initialization occurs only before the first mini-batch of the first epoch\n",
    "    - need to maintain a good distribution across all mini-batches and all epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This motivates the introduction of a new class of layer-types: Normalization layers\n",
    "\n",
    "- These layer types attempt to keep the distribution of $\\y_{\\llp,j}$\n",
    "normalized through all layers $\\ll$ and training steps\n",
    "- They become most necessary for *very deep* (large number of layers) networks\n",
    "\n",
    "Normalization layers were one of the innovations that advanced Deep Learning\n",
    "by enabling learning in networks of extreme depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[Batch Normalization paper](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "If our goal is to ensure that the output distribution of each layer is identical\n",
    "- there is a simple solution\n",
    "- apply a standardization layer to each activation output to ensure mean $0$, variance outputs\n",
    "\n",
    "Batch Normalization is a layer type that essentially carries out this standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is called  *batch* normalization because\n",
    "- the mean and variance used for standardization is computed on a *per batch* basis\n",
    "- hopefully, each batch's statistics are an approximation of the statistics of the true training distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Experimental results show that the technique:\n",
    "- facilitates the use of much higher learning rates, thus speeding training.  Accuracy is not lost.\n",
    "- facilitates the use of saturating activations functions (e.g., $\\tanh$ and sigmoid) which otherwise are subject to vanishing/exploding gradients.\n",
    "- acts as a regularizer; reduces the need for Dropout\n",
    "    - L2 regularization (weight decay) has *no* regularizing effect when used with Batch Normalization !\n",
    "        - [see](https://arxiv.org/abs/1706.05350)\n",
    "        - L2 regularization affects scale of weights, and thereby learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Details\n",
    "\n",
    "Consider a layer $\\ll$ computing\n",
    "$$\n",
    "\\y_\\llp = a_\\llp( f_\\llp(\\y_{(\\ll-1)}, W_\\llp) )\n",
    "$$\n",
    "\n",
    "We use \n",
    "- superscript $i$ to denote the value of $\\y_\\llp$ when the Neural Network input\n",
    "is example $i$.\n",
    "- $m_B$ to denote the number of examples in a mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Batch Normalization of layer $\\ll$ output is defined by the following equations\n",
    "- producing standardized output $\\z^\\ip$ given \n",
    "Neural Network input\n",
    "is example $i$.\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\y_\\llp^\\ip} & \\quad  \\text{Batch mean}\\\\\n",
    "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\y_\\llp^\\ip - \\mathbf{\\mu}_B)^2} & \\quad \\text{Batch variance} \\\\\n",
    "3.\\quad & \\hat\\y_\\llp^\\ip = \\dfrac{\\y_\\llp^\\ip - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}} & \\quad \\text{Standardize } \\y_\\llp^\\ip\\\\\n",
    "4.\\quad & \\mathbf{z}^\\ip = \\gamma \\hat\\y_\\llp^\\ip + \\beta  & \\quad \\text{De-Standardize } \\hat\\x^\\ip  \\text{ with learned mean and variance}\\\\\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notes**\n",
    "- The $\\epsilon$ in the denominator is there solely to prevent \"divide by 0\" errors\n",
    "- The statistics are computed for each feature\n",
    "    - so $\\mu_B$ and $\\sigma_B$ are vectors of length $n_\\llp$\n",
    "- Normalization is sometimes defined with respect to \n",
    "    - the pre-activation outputs $f_\\llp(\\y_{(\\ll-1)}, W_\\llp)$\n",
    "    - rather than the activation outputs $\\y_\\llp$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first 3 steps is simple standardization by the mean and variance of the mini-batch.\n",
    "\n",
    "But: after we obtain standardized $\\hat\\y_\\llp^\\ip$ we \n",
    "- shift the mean from $0$ to $\\beta$\n",
    "- scale the standard deviation by $\\gamma$\n",
    "- to obtain the final output $\\z^\\ip$\n",
    "\n",
    "where $\\beta$ and $\\gamma$ are **learned parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is the point of changing from the distribution to mean $\\beta$, variance $\\gamma^2$ ?\n",
    "\n",
    "It can't hurt:\n",
    "- if the optimal values are $\\beta = 0$ and $\\gamma = 1$ so that $\\z^\\ip = \\hat\\y_\\llp^\\ip$\n",
    "    - these are the values that will be learned\n",
    "- if there is an advantage to the shift: it will be learned    \n",
    "    - if we are normalizing the pre-activation inputs (rather than the activation outputs)\n",
    "    - the standardized $\\hat\\y_\\llp^\\ip$ may wind up in a linear region of the activation function\n",
    "    - resulting in a linear transformation\n",
    "    - shifting may move the result to a non-linear part of the activation function\n",
    "        - resulting in a non-linear transformation: more powerful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At inference time\n",
    "- all the parameters ($\\mu_B, \\sigma_B, \\beta, \\gamma$) are fixed\n",
    "- the $\\mu_B$ and $\\sigma_B$ used for each batch are *identical* across mini-batches\n",
    "    - are \"population\" statistics $\\mu, \\sigma$ computed over the full training dataset\n",
    "    - usually by a decaying moving average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Keras has a `BatchNormalization` layer type to perform Batch Normalization of its inputs.\n",
    "\n",
    "\n",
    "\n",
    "Using our running example as illustration\n",
    "- The following chart shows the effect of adding a `BatchNormalization` layer after the first few `Dense` layers\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><center>RELU</center></td> \n",
    "        <td><center>SIGMOID</center></td>\n",
    "        <td><center>TANH</center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"3\">\n",
    "            <img src=\"images/tnn_loss_and_acc_bn.png\">\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We observe that `BatchNormalization`\n",
    "- when used on the network with Sigmoid activations\n",
    "- significantly improves the Loss and Accuracy compared to the network without it\n",
    "- We can see the output of the `BatchNormalization` layer roughly maintains\n",
    "    - mean $0$ and unit variance across epochs\n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><center>RELU</center></td> \n",
    "        <td><center>SIGMOID</center></td>\n",
    "        <td><center>TANH</center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"3\">\n",
    "            <img src=\"images/tnn_activations_bn.png\">\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "Batch Normalization is almost a \"magic bullet\" for training\n",
    "- avoids problems caused by covariate shift\n",
    "- and *also* facilitates higher learning rates\n",
    "    - accelerating training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are some theories as to why it enables higher learning rates\n",
    "- the weights are updated by the product of the learning rate and the derivatives\n",
    "    - by controlling the size of the derivatives\n",
    "    - we can use a larger learning rate\n",
    "    - without increasing the size of the update\n",
    "- the $\\mu_B$ and $\\sigma_B$ of each mini-batch are *noisy estimates* of the training set's mean and variance\n",
    "    - since these values vary by mini-batch, noise is added to training\n",
    "    - act as a form of regularization\n",
    "        - forcing the Neural Network's weights to not overly-adapt to each batch\n",
    "        - similar to the regularization effect seen in Dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final words\n",
    "\n",
    "There are several other common forms of Normalization:\n",
    "- [Layer Normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization)\n",
    "- [Group Normalization](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GroupNormalization)\n",
    "\n",
    "In practice, to mitigate the cost to training and inference\n",
    "- normalization layers are only applied every few layers, not for every layer\n",
    "\n",
    "There is also some controversy over the wisdom of [using both Batch Normalization and Dropout](https://arxiv.org/pdf/1801.05134.pdf). Essentially:\n",
    "- Dropout causes the batch statistics computed at training time\n",
    "- to differ from the population statistics of the training dataset\n",
    "- which means that the test dataset does not reflect the same distribution as was used in training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BatchNormalization and Transfer Learning (subtlety)\n",
    "\n",
    "This is a subtle and technical point involving using\n",
    "- Pre-trained models\n",
    "- In Transfer Learning (a subsequent module)\n",
    "\n",
    "You might want to revisit this section once you start to use Pre-Trained models and Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transfer Learning in a nutshell:\n",
    "- freeze the weights of a Pre-Trained \"Base\" model that has been trained on a Source Task, using many examples\n",
    "- train *only* newly added layers in order to adapt the augmented model to a Target task\n",
    "    - using a *small* number of training examples for the Target Task\n",
    "- this is an effective way to solve a new Target Task with less effort than training a model from scratch\n",
    "    - \"Transfer\" the knowledge encoded in the parameters during training of the Source Task\n",
    "        - to the Target Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue arises from freezing the base model when it contains normalization layers\n",
    "- Batch Normalization layers are unique in that they contain *non-trainable* weights\n",
    "    - in addition to the usual trainable weights\n",
    "    - the non-trainable weights are the batch statistics\n",
    "        - computed values, but not part of the trainable weights\n",
    "\n",
    "The question becomes: what happens to the non-trainable weights during training when the layer is frozen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The answer revolves around a technical distinction between *inference mode* and *frozen state*\n",
    "- \"Inference mode\" is what happens when the model is not training\n",
    "    - For `BatchNormalization`: uses the population statistics during *inference*\n",
    "- \"Frozen state\" is what happens during training when a layer's weights are frozen; `layer.trainable = False`\n",
    "    - recall: the batch statistics are *not* trainable weights, so should not be affected by `layer.trainable`\n",
    "        - might expect batch statistics to always be used\n",
    "    - For `BatchNormalization`, it was decided instead\n",
    "        - `layer.trainable = False` means the layer operates in *inference model*\n",
    "            - even when the model is training (i.e, not performing inference)\n",
    "        -  results in using the population statistics during *training*\n",
    "            - rather than batch statistics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One reason for this decision\n",
    "- in Transfer Learning, we usually set `trainable = False` for the *entire model*\n",
    "    - which recursively sets `trainable = False` for all the model layers\n",
    "        - including `BatchNormalization`\n",
    "    - rather than freezing layers individually\n",
    "    - it is therefore easy to accidentally set `trainable = False` for `BatchNormalization`\n",
    "    \n",
    "So the decision was made to treat `trainable = False` for `BatchNormalization` layers\n",
    "- in a non-standard way\n",
    "- to capture the user's intent during Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Maintaining good properties of layer inputs throughout the depth of a multi-layer network\n",
    "is like priming a pump.\n",
    "\n",
    "Proper priming helps our learning to flow smoothly.\n",
    "\n",
    "We explored some of the stumbling blocks to learning (weight update) along with their solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
