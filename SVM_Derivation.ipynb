{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM Cost function derived from Constrained Optimization\n",
    "\n",
    "Here is an alternate (and slightly more mathematical) derivation of the SVM Cost function.\n",
    "\n",
    "Recall our dual objectives\n",
    "- Maximize margin (width of buffer zone)\n",
    "- Minimize classification loss (incorrectly classified or in-buffer training examples)\n",
    "\n",
    "The natural way to express these objectives is as a Constrained Optimization problem\n",
    "- Maximize margin\n",
    "- subject to not violating a boundary constraint (all examples on correct side of margin boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We already showed\n",
    " how maximizing the margin was equivalent to minimizing the margin penalty\n",
    "$$\n",
    "\\frac{1}{2} \\Thetam^T \\cdot \\Thetam\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each per-example Classification Loss\n",
    "$$\n",
    "\\max{} \\left( 0,  1 - \\transy^\\ip * s(\\hat{\\x}^\\ip ) \\right)\n",
    "$$\n",
    "\n",
    "is equivalent to an inequality constraint\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    " 1 - \\transy^\\ip * s(\\hat{\\x}^\\ip) \\le 0 \\\\\n",
    " \\transy^\\ip * s(\\hat{\\x}^\\ip) \\ge 1  & \\text{re-arranging the terms} \\\\\n",
    " \\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we can express the Loss Function minimization as a constrained optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\text{minimize } \\frac{1}{2} \\Thetam^T \\Thetam \\\\\n",
    "\\text{subject to }  \\transy^\\ip * s(\\hat{\\x}^\\ip) \\ge 1 \\text{for } i=1,\\ldots,m\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The objective\n",
    "$$\n",
    "\\frac{1}{2} \\Thetam^T \\Thetam\n",
    "$$\n",
    " is quadratic so this is a Quadratic Optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solving Constrained Optimization: LaGrangian multipliers\n",
    "\n",
    "It is beyond the scope of this course, but one way to solve constrained minimization\n",
    "is to create an objective function $\\loss$ that is the sum of\n",
    "- the function to minimize\n",
    "- $\\lambda$ times each constraint (when it is rewritten in a form where the inequality is with respect to $0$)\n",
    "\n",
    "The $\\lambda$ terms are called *Lagrangian multipliers*.\n",
    "\n",
    "They serve as penalties when a constraint is violated (i.e., is greater than $0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The (per example) objective function $\\loss$ becomes\n",
    "$$\n",
    " \\frac{1}{2} \\Thetam^T \\Thetam + \\lambda * ( \\max{}(0, - 1 * \\transy^\\ip * s(\\x^\\ip)))\n",
    "$$\n",
    "\n",
    "which is equal to the SVM Cost function that we constructed in the ad hoc fashion\n",
    "if we let $\\lambda = C$.\n",
    "\n",
    "Recall $C$ expressed the relative weight between the Margin Penalty and Classification Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "- You will sometimes see the max replaced by a \"slack\" variable $\\xi^\\ip$\n",
    "$$\\xi^\\ip = \\max{}(0, - 1 * \\transy^\\ip * s(\\x^\\ip))$$\n",
    "\n",
    "    This is a way of turning an inequality \n",
    "$$\n",
    "\\transy^\\ip * s(\\hat{\\x}^\\ip) \\ge 1\n",
    "$$\n",
    "into an equality\n",
    "    - i.e., the \"slack\" is the amount of the difference of the expression from being equal\n",
    "- Technically, there should be a separate Lagrangian for each constraint\n",
    "    - should write $\\lambda^\\ip$ for $i = 1 \\le i \\le m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Asides**\n",
    "\n",
    "- Write penalty with the $\\frac{1}{2}$ in front so that the derivative with respect to $\\w$ is $\\w$.\n",
    "- The Regularization Penalty is the same as the Margin Penalty of our prior derivation\n",
    "- The relative weight between Classification Loss and the Regularization Penalty is $\\lambda$\n",
    "    - The Regularization Penalty is $\\lambda$ times more important than Classification Loss\n",
    "    - Prior derivation: Classification Loss was $C$ times as important as Regularization Penalty\n",
    "    - so $C = \\frac{1}{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Support Vector Machines: derivation via landmark similarity\n",
    "\n",
    "We now show another derivation of the Support Vector Machine.\n",
    "\n",
    "The SVM will apply a particular kind of transformation $\\phi$ to $\\x$ before fitting a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Landmarks and the Similarity transformation\n",
    "\n",
    "Let us pick a set of distinguished points $\\{ \\lmk^{(1)}, \\lmk^{(2)}, \\ldots \\}$ in the input domain.\n",
    "\n",
    "We will refer to these distinguished points as \"landmarks\" because we will use\n",
    "them as reference points from which we will measure the similarity (inverse of distance) of each example $\\x^\\ip$ in the training set.\n",
    "\n",
    "In particular, let us choose $n'$ landmarks and let\n",
    "$$\n",
    "K(\\x, \\lmk^\\ip)\n",
    "$$\n",
    "be a measure of similarity between vector $\\x$ and the $i^{th}$ landmark.\n",
    "\n",
    "$K$ will be referred to as a \"similarity function\" or \"kernel\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the transformation of $\\x^\\ip$ into\n",
    "$$\n",
    "\\hat\\x^\\ip = [ K(\\x^\\ip, \\lmk^{(1)}), K(\\x^\\ip, \\lmk^{(2)}), \\ldots, K(\\x^\\ip, \\lmk^{(n')}),]\n",
    "$$\n",
    "is a representation of the original $\\x^\\ip$ into a \"similarity\" vector.\n",
    "\n",
    "The transformed features $\\hat{\\x}^\\ip$ is of length $n'$, each element\n",
    "representing the distance of $\\x^\\ip$ to one landmark.\n",
    "\n",
    "We will do linear classification on these transformed features $\\hat\\x$ rather than the original $\\x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The linear classifier creates a hyperplane to separate Positive and Negative examples\n",
    "by using the dot product to create a score \n",
    "$$s(\\hat{\\x}^\\ip) = \\hat{\\Theta}^T \\cdot  \\hat{\\x}^\\ip$$\n",
    "based on transformed $\\hat{\\x}^\\ip$\n",
    "\n",
    "This score will determine the prediction.\n",
    "\n",
    "Score:\n",
    "$$\n",
    "s(\\hat{\\x}^\\ip) = \\hat{\\Theta}^T \\cdot \\hat{\\x}^\\ip + b\n",
    "$$\n",
    "\n",
    "Score to prediction:\n",
    "$$\n",
    "\\hat{\\y}^\\ip = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } s(\\hat{\\x}^\\ip) < 0 \\\\\n",
    "1 & \\text{if } s(\\hat{\\x}^\\ip) \\ge 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Note** \n",
    "\n",
    "- $\\hat\\Theta$ and $\\hat{\\x}$ must have the same length\n",
    "- The $\\hat\\Theta$ in this derivation is thus **very different** than the $\\Theta$ in the original derivation\n",
    "    - in this derivation the length of $\\hat\\Theta$ is $n'$ (number of examples) **not** $n$ (number of original features)\n",
    "    - the elements of this $\\hat\\Theta$ correspond to *landmarks* **not** features in the original dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So once again, the dot product is doing a form of \"pattern matching\" of features,\n",
    "but now \n",
    "- we use transformed features: similarity to landmarks\n",
    "- the pattern is identifying the relative importance of being similar to each landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far, very similar to Logistic Regression: the score determines the prediction.\n",
    "\n",
    "One difference is that Logistic Regression converts score to  probability\n",
    "via a sigmoid function:\n",
    "$$\n",
    "\\hat{p}^\\ip = \\sigma( s(\\x^\\ip))\n",
    "$$\n",
    "This probability is needed mainly for the Cost function for Logistic Regression (Binary Cross Entropy)\n",
    "but may be useful as an informative output as well.\n",
    "\n",
    "We shall soon seen the main difference from Logistic Regression: Hinge Loss\n",
    "rather than Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the landmarks\n",
    "\n",
    "**TO DO** Picture\n",
    "\n",
    "How do we choose the landmarks ?  How many do we choose ?\n",
    "\n",
    "Let's choose each of the $m$ examples in the training set as a landmark so\n",
    "- $n' = m$\n",
    "- $\\lmk^\\ip = \\x^\\ip$.\n",
    "\n",
    "This might initially seem to be a large number of landmarks.  \n",
    "\n",
    "Is it possible to choose fewer ?\n",
    "\n",
    "Yes, and we will let Machine Learning decide which ones matter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
