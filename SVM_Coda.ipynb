{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM for Regression\n",
    "\n",
    "SVM's can be used not only for Classification but for Regression as well.\n",
    "\n",
    "For Classification, the Loss function tries to fit all training examples *outside* the buffer zone.\n",
    "\n",
    "For Regression, the Loss function tries to fit all training examples *within* the buffer zone.\n",
    "\n",
    "- This is analogous to forcing training examples to be close to the fitted line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importance of normalization of $\\x$\n",
    "\n",
    "Imagine that the various components of $\\x$ are on vastly different scales\n",
    "- suppose $\\x_1$ is in \"billions of units\" (so $\\x_1$ is small) but $\\x_2$ is in single units (large)\n",
    "\n",
    "Then parameter $\\Theta_1$ (which multiplies $\\x_1$) will likely be much smaller than $\\Theta_2$.\n",
    "\n",
    "The Margin Penalty, which tries to reduce the value of elements of $\\Theta$ will thus\n",
    "be more likely to reduce $\\Theta_2$ than $\\Theta_1$ for no reason other than the disparate scales.\n",
    "\n",
    "As we saw in our lecture on transformations, this is a danger with loss/cost functions that\n",
    "include a penalty for parameters magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The disparate scales of features in $\\x$ can also affect some transformations.\n",
    "\n",
    "For the Gaussian RBF, the 2-norm will be dominated by the larger dimension, almost to the exclusion of smaller dimensions.\n",
    "\n",
    "Thus, if the chosen $\\phi$ is sensitive to scale (such as the Gaussian RBF) it's important to normalize the training set (across each dimension of features) so that\n",
    "all features (elements of $\\x$) are on similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM Drawbacks\n",
    "\n",
    "## Complexity\n",
    "The SVM may utilize up to $m$ landmarks from the vectors among the training set.\n",
    "\n",
    "These landmarks are referred to as \"support vectors\".\n",
    "\n",
    "As $m$ may be very large\n",
    "- SVM models may consume a lot of memory (in the test phase, not just when being fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The choice of the complexity of a kernel depends on the relationship\n",
    "between $m$, the number of training examples, and $n$, the number of features.\n",
    "\n",
    "- When $m$ is large (and large relative to $n, m \\gg n$) there is a lot of data on which to fit a complicated kernel.\n",
    "    - however, when $m$ is really large, this can be expensive\n",
    "- When $m$ is small, the lack of data for fitting suggests a simple, linear kernel or logistic regression\n",
    "- when $n$ is small, simpler Kernels or Logistic Regression is suggested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "- The SVC Optimization problem is a type of Quadratic Programming problem\n",
    "    - In the \"primal\" form of the equation: there are $m$ (number of examples) constraints\n",
    "    - In the \"dual\" form of the equation, there are $n$ (number of features) constraints\n",
    "- Kernels work on the dual form of the equation\n",
    "    - So if $n'$, the number of transformed features, is much bigger than $m$, kernels may cost more than pre-processing transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SVM in `sklearn`\n",
    "\n",
    "`sklearn` has several SVM algorithms (using different optimization techniques) that, mathematically,\n",
    "should yield equivalent results.\n",
    "\n",
    "The difference is that some optimize for time (faster solution) versus space (ability to handle bigger\n",
    "training sets) versus being more general solvers.\n",
    "\n",
    "According to Geron (page 194), the following should be equivalent\n",
    "- `LinearSVC(C=1, loss=\"hinge\")`\n",
    "- `SVC(C=1,kernel=\"linear\")`\n",
    "- `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final words\n",
    "\n",
    "The SVM is very powerful, but it has a lot of moving parts\n",
    "- Hinge Loss\n",
    "- Large Margin Classification\n",
    "- Kernel\n",
    "\n",
    "The power of the SVM comes in the tight integration of all these parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Main take-aways\n",
    "- The SVM offers you built-in transformations to help make the data set closer to being Linearly Separable\n",
    "- You can use the kernel function even without understanding the transformation $\\phi$ that corresponds to it\n",
    "- Some kernel functions have hyper-parameters\n",
    "    - The SVM can solve for the best hyper-parameter\n",
    "- SVM's can also be used for Regression tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Preview of Deep Learning\n",
    "\n",
    "Two SVM features\n",
    "- The \"max\" function of the Hinge Loss\n",
    "- The semi-automatic construction of synthetic features (kernels)\n",
    "\n",
    "will be key features of a different set of models using Deep Learning \n",
    "that are the focus of the second half of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
