{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training neural networks: the nitty-gritty\n",
    "\n",
    "We know that \n",
    "the weights $\\W^*$ that minimize the average loss\n",
    "\n",
    "$$\n",
    "\\W^* = \\argmin{W} L(\\hat{\\y},\\y; \\W)\n",
    "$$\n",
    "\n",
    "can be found by using Back Propagation\n",
    "([see the earlier module](Training_Neural_Network_Backprop.ipynb))\n",
    "to compute the derivatives needed by the Gradient Descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So training a Neural Network sounds simple enough.\n",
    "\n",
    "But we also recalled several \"AI Winters\" in which progress in Deep Learning stalled.\n",
    "\n",
    "History indicates that Training is more complex than it appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will illustrate the complexities via a running example\n",
    "- Training a binary classifier\n",
    "- On the dataset shown on the left\n",
    "    - Two features: $x_0, x_1$\n",
    "    - Targets: 0, 1 \n",
    "- Using the Neural Network architecture shown on the right\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><center><strong>Data</strong></center></td>\n",
    "        <td><center><strong>Neural Network</strong></center></td>\n",
    "     </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/tnn_data.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/tnn_arch.png\">\n",
    "         </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effect of different Activation functions\n",
    "\n",
    "Let's see the effect of choosing among ReLU, Sigmoid and Tanh activation functios\n",
    "- Keeping the architecture identical\n",
    "- but changing the activation functions of the `Dense` layers\n",
    "    - the Classifier `Dense` layer always uses the sigmoid (as a classifier must)\n",
    "\n",
    "We see a difference in Loss and Accuracy\n",
    "- note the difference in the scale of the vertical axes between plots\n",
    "\n",
    "<img src=\"images/tnn_loss_and_acc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effect of weight initialization\n",
    "\n",
    "Training is the process of discovering optimal weights/parameters for the components of the Neural Network.\n",
    "- updating an initial choice via Gradient Descent\n",
    "\n",
    "Yet, we have not specified how to initialize the weights.\n",
    "\n",
    "Does it matter ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the diagram above, we saw the Loss and Accuracy when initializing weights\n",
    "- according to a Random Normal distribution with mean 0 and unit variance\n",
    "\n",
    "Here are the identical plots when initializing weights\n",
    "- to all zero\n",
    "\n",
    "<img src=\"images/tnn_loss_and_acc_zero_init.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The loss and accuracy of **all** the networks is notably worse than the Random Normal initialization\n",
    "- The Loss hardly changes with training: the network is not \"learning\"\n",
    "- The Accuracy is not better than a coin flip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Understanding what happens during training\n",
    "\n",
    "As we saw from our experiments\n",
    "- choices that seem minor\n",
    "    - activation function\n",
    "    - weight initialization\n",
    "- can have a major impact on the success of training a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now spend some time investigating the causes, and solutions, to the difficulty of training networks.\n",
    "\n",
    "Broadly speaking the issues are\n",
    "- Gradients becoming zero or infinity, inhibiting learning (weight updates in Gradient Descent)\n",
    "- Proper scaling of the inputs\n",
    "- Initialization of learnable weights\n",
    "- Making sure that the proper scaling of inputs continues to each layer, not just the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanishing and Exploding Gradients\n",
    "\n",
    "Although Backpropagation is mechanically simple, there are some mathematical subtleties.\n",
    "\n",
    "Let's explore the problem of\n",
    "[Vanishing and Exploding Gradients](Vanishing_and_Exploding_Gradients.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initializing and maintaining layer inputs\n",
    "\n",
    "Neural Networks are sensitive to the scale of the layer inputs.\n",
    "\n",
    "Creating the correct situation to learn is the subject of [Scaling and Initialization](Training_Neural_Networks_Scaling_and_Initialization.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improving trainability\n",
    "\n",
    "Apart from the mathematical issues of preventing activations and gradients from exploding/vanishing, there are many ways to make training successful.\n",
    "\n",
    "Let's explore techniques for [Improving trainability](Training_Neural_Networks_Tweaks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How big should my NN be ?\n",
    "\n",
    "There is a paradox in building Neural Networks:\n",
    "- Start off training an overly large NN (many units)\n",
    "- Many units turn out to be \"dead\": near zero weights\n",
    "- Reduce the number of units\n",
    "- Can't train !\n",
    "\n",
    "Given a fixed number of layers: it is easier to train a big NN than a small one.\n",
    "\n",
    "\"Somewhere in this big mess must be something valuable\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>\"Big\" NN -- some nodes are dead</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Dropout_NN_wo_dropout.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>\"Big\" NN after dead nodes have been pruned</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Dropout_NN_w_dropout.png\"</td>\n",
    "    </tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)\n",
    "is an interesting paper that addresses this issue.\n",
    "\n",
    "For now:\n",
    "- Use bigger than necessary NN's\n",
    "- With regularization to \"prune\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We sometimes take training Neural Networks for granted.\n",
    "\n",
    "After all, Gradient Descent seems like a simple procedure.\n",
    "\n",
    "It turns out that there are *many* subtleties.\n",
    "\n",
    "Uncovering and solving the subtle problems were the key contributions in the recent rapid advance\n",
    "of Deep Learning.\n",
    "\n",
    "Without them, we'd still be living in \"AI Winter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
