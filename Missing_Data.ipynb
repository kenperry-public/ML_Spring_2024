{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "$\n",
    "\\newcommand{\\tx}{\\x^{(m')}}\n",
    "\\newcommand{\\txi}{\\hat{\\x}^{(m')}}\n",
    "\\def\\prox#1{\\mathcal{prox}(#1)}\n",
    "\\def\\proxtx#1{\\prox{\\tx, #1}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "MOVIE_DIR=\"./images\"\n",
    "\n",
    "CREATE_MOVIE = False # True if you have ffmpeg installed\n",
    "\n",
    "import training_models_helper\n",
    "%aimport training_models_helper\n",
    "\n",
    "tmh = training_models_helper.TrainingModelsHelper()\n",
    "\n",
    "import mnist_helper\n",
    "%aimport mnist_helper\n",
    "\n",
    "mnh = mnist_helper.MNIST_Helper()\n",
    "\n",
    "import class_helper\n",
    "%aimport class_helper\n",
    "\n",
    "clh= class_helper.Classification_Helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Missing features\n",
    "\n",
    "What if our data is not perfect ?\n",
    "\n",
    "What do we do about examples with missing features\n",
    "- training examples\n",
    "- test examples\n",
    "\n",
    "We have thus far being treating this as an annoyance\n",
    "- problem is important\n",
    "- far from simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The simplest thing to do would be to drop the example\n",
    "- can't do it with a test example\n",
    "- reduces amount of data in training\n",
    "\n",
    "We could alternatively, drop the feature entirely\n",
    "- the features missing among several examples may be disjoint\n",
    "\n",
    "Either way, losing training data is not desirable, particularly for small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We will motivate the problem and illustrate the issues\n",
    "- We will examine naive solutions\n",
    "    - almost always bad !\n",
    "- We will show an interesting solution using Random Forests\n",
    "- Preview of clustering methods\n",
    "\n",
    "The term **imputation** refers to creating a substitute for the missing value of a feature\n",
    "in one example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To frame our discussion\n",
    "\n",
    "Let \n",
    "- Let $f$ denote the index of a feature\n",
    "- $\\tx$ be an example (either training or test) with missing feature $\\tx_f$\n",
    "- $\\txi$ be the imputed valued for $\\tx$\n",
    "\n",
    "As usual let $\\X, \\y$ be our labeled training examples.\n",
    "\n",
    "$$\n",
    "\\{ (\\x^\\ip, \\y^\\ip) | 1 \\le i \\le m \\} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive methods for imputation\n",
    "\n",
    "## Magic numbers\n",
    "Let's start with a truly awful method: set $\\txi_f$ to a  \"magic number\"\n",
    "- 0\n",
    "- -999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why is the magic number awful ?\n",
    "\n",
    "Consider a training set representing the population of NYC, with features Weight, Height, Age, etc.\n",
    "\n",
    "Suppose Weight, which is at index $f$ of example vector $\\tx$ is missing.\n",
    "\n",
    "Setting $\\txi_f = 0$ is awful because the imputed value is not likely\n",
    "- $\\pr{\\tx_f = 0} \\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mean, median, percentile\n",
    "\n",
    "How about something more likely, like the mean or median ?\n",
    "\n",
    "Better\n",
    "- $\\pr{\\tx_f = \\bar{\\x}_f} > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Still not perfect.\n",
    "- What if $\\pr{\\x_f}$ were a bi-modal distribution\n",
    "    - lots of examples with extreme values, few in the middle\n",
    "    \n",
    "So mean and median are better than magic numbers in many situations but not all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even worse:\n",
    "\n",
    "Suppose example $\\tx$ is an infant: is $\\bar{\\x}_f$ still reasonable ?\n",
    "- $\\pr{\\tx_f = \\bar{\\x}_f | \\tx_{\\text{Age}} < 1} \\approx 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the mean, median etc. \n",
    "- provides a reasonable imputation in a univariate sense\n",
    "- provides a less reasonable imputation in a multivariate sense\n",
    "    - conditional on other features like Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imputation depends on **how** the imputed value is used\n",
    "\n",
    "Less obvious is that $\\txi_f$ will be used for training some model.\n",
    "\n",
    "So perhaps we should consider how the model that we are going to fit uses $\\txi_f$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate, suppose the model  uses\n",
    "the dot product to measure similarity among examples\n",
    "- a variant of KNN\n",
    "\n",
    "Knowing this, we can show that different choices of $\\txi_f$ influence the similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "S1 = pd.Series( { \"a\": 4,               \"d\": 5, \"e\":1 })\n",
    "S2 = pd.Series( { \"a\": 5, \"b\":5, \"c\": 4})\n",
    "S3 = pd.Series( {                       \"d\": 2, \"e\": 4, \"f\": 5})\n",
    "\n",
    "df = pd.DataFrame( [S1, S2, S3], index=[\"A\", \"B\", \"C\"])\n",
    "\n",
    "\n",
    "A = df.loc[\"A\",:]\n",
    "B = df.loc[\"B\",:]  \n",
    "C = df.loc[\"C\",:]\n",
    "\n",
    "\n",
    "def sim(A, B):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity of vectors A and B\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    A, B: ndarrays of same length\n",
    "    \"\"\"\n",
    "    return (A * B).sum()/( np.sqrt( (A*A).sum() ) * np.sqrt( (B*B).sum() ) )\n",
    "\n",
    "def compare_subs(X, Y):\n",
    "    \"\"\"\n",
    "    Compare various ways of filling in missing values \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y: ndarrays of equal length\n",
    "    \"\"\"\n",
    "    (Xp, Yp) = (X.fillna(0), Y.fillna(0))\n",
    "    print(\"\\tSubstitute 0: similarity= {s:0.2f}\".format(s= sim( Xp, Yp )) )\n",
    "\n",
    "    # Substitute mean of each student\n",
    "    (Xp, Yp) = (X.fillna( X.mean() ), Y.fillna( Y.mean() ))\n",
    "    print(\"\\tSubstitute w/feature mean: similarity= {s:0.2f}\".format(s= sim( Xp, Yp  )) )\n",
    "\n",
    "    # Center data, then substitute 0\n",
    "    # Centered mean tells a different story (b/c of magnitudes of entries ?)\n",
    "    # n.Y., Cov(X,Y) = E(X*Y) - E(X)E(Y) so is X, Y not centered, dot product is not Cov(X,Y)\n",
    "    (Xp, Yp) = ( (X - X.mean()).fillna(0), (Y - Y.mean()).fillna(0) )\n",
    "    print(\"\\tCenter, then Substitute 0: similarity= {s:0.2f}\".format(s= sim( Xp, Yp  )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    d    e    b    c    f\n",
       "A  4.0  5.0  1.0  NaN  NaN  NaN\n",
       "B  5.0  NaN  NaN  5.0  4.0  NaN\n",
       "C  NaN  2.0  4.0  NaN  NaN  5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vs B\n",
      "\tSubstitute 0: similarity= 0.38\n",
      "\tSubstitute w/feature mean: similarity= 0.94\n",
      "\tCenter, then Substitute 0: similarity= 0.09\n",
      "A vs C\n",
      "\tSubstitute 0: similarity= 0.32\n",
      "\tSubstitute w/feature mean: similarity= 0.87\n",
      "\tCenter, then Substitute 0: similarity= -0.56\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "\n",
    "print(\"A vs B\")\n",
    "compare_subs(A,B)\n",
    "\n",
    "print(\"A vs C\")\n",
    "compare_subs(A,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A versus B\n",
    "    - missing values are paired against relatively high values\n",
    "        - Substituting $0$ (a low value) reduces similarity\n",
    "        - Substituting mean (a relatively high value) increase similarity\n",
    "            - A is a tougher rater: A.mean() < B.mean()\n",
    "                \n",
    "    - in the end, A and B had only a *single* true point of comparison (\"a\")\n",
    "        - you made up the similarity\n",
    "\n",
    "- A versus C:\n",
    "    - *NO* feature \"a\" in common !\n",
    "    - But at least A and B are closer than A and C for some substitutions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cosine similarity\n",
    "- is a *scale dependent* measure\n",
    "    - so centering, scaling matter\n",
    "    - Analogy\n",
    "        - difference between Covariance and Correlation\n",
    "            - Correlation is Covariance of normalized (scaled) variables\n",
    "\n",
    "The choice of $\\txi_f = 0$ is **not** neutral if the data is not centered\n",
    "- e.g., if $0$ is the minimum of $\\x_f$, rather than the average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive methods for imputation\n",
    "\n",
    "Hopefully the preceding examples illustrated some issues in imputation.\n",
    "\n",
    "Can we do better ?\n",
    "\n",
    "Let $\\x_{\\bar{f}}$ denote the vector of features *excluding* the one at index $f$.\n",
    "\n",
    "We can frame the imputation problem as finding\n",
    "$$\n",
    "\\pr{\\tx_f | \\tx_{\\bar{f}}}\n",
    "$$\n",
    "\n",
    "That is: find likely values for the missing feature, *given* values for the non-missing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we do this ?\n",
    "\n",
    "Machine Learning to the rescue !\n",
    "- fit a model on the subset of training examples \n",
    "    - that *have* feature $f$ (used as target)\n",
    "- use the model to predict $\\txi_f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simple predictive imputation\n",
    "Some ideas\n",
    "- Naive Bayes\n",
    "    - Assumption of distribution of features can compensate for missing features\n",
    "- Regression\n",
    "    - $\\x_f = \\Theta^T \\x_{\\bar{f}}$\n",
    "        - feature $f$ as a function of the other features\n",
    "    - $\\x_f = \\Theta^T \\z$\n",
    "        - $\\z$ may be features, not present in $\\x$, that are believed to be correlated with $\\x_f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proximity based imputation\n",
    "\n",
    "A proximity based method\n",
    "- creates a proximity (opposite of distance) measure $\\proxtx{\\x^\\ip}$ between $\\tx$ and training example $\\x^\\ip$\n",
    "-  $\\txi_f$ is the proximity weighted average of the values of the feature in the training set\n",
    "$$\n",
    "\\txi_f = \\sum_{i=1, i \\ne m'}^m { \\proxtx{\\x^\\ip} \\x^\\ip_f }\n",
    "$$\n",
    "\n",
    "That is\n",
    "- the missing value should be similar to the feature value\n",
    "in training examples  \"similar\" to $\\tx$.\n",
    "\n",
    "The definition of proximity (similarity) will vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note** \n",
    "\n",
    "For categorical $\\tx_f$  use the most frequent non-missing value\n",
    "- where the frequency is weighted by proximities.\n",
    "\n",
    "The method works for multiple missing features but we illustrate with a single one for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitations\n",
    "\n",
    "For the imputation of \n",
    "$$\n",
    "\\pr{\\tx_f | \\tx_{\\bar{f}}}\n",
    "$$\n",
    "\n",
    "we are implicitly assuming that if feature vectors $\\x^\\ip, \\x^{(i')}$ are \"similar\" then\n",
    "so are their targets\n",
    "$$\\y^\\ip \\approx \\y^{(i')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With that limitation in mind there are related methods\n",
    "- Clustering\n",
    "    - find groups of examples with common features\n",
    "        - K-means\n",
    "    - PCA, Recommender systems\n",
    "        - Unsupervised Machine Learning: Preview of coming lecture !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest proximity method for missing data\n",
    "\n",
    "There is an interesting method for using a Random Forest to impute missing data.\n",
    "\n",
    "It is interesting because proximity is determined by both the features *and* the target\n",
    "so we are modeling\n",
    "\n",
    "$$\n",
    "\\pr{\\x_f | \\y,\\x_{\\bar{f}}}\n",
    "$$\n",
    "\n",
    "That is, it fits a model of $\\x_f$ given the features other than $f$ **and** the target.\n",
    "\n",
    "A test example with missing features doesn't have a target; \n",
    "we will see how this method adapts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Missing feature in Training example\n",
    "\n",
    "We will use a Random Forest to define a proximity measure.\n",
    "\n",
    "[Missing Value Imputation using Random Forest](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#missing1)\n",
    "\n",
    "- Initialization\n",
    "    - Set $\\txi_f$ to a \"reasonable\" guess\n",
    "        - Continuous: mean, median\n",
    "        - Categorical: most frequent\n",
    "    - Create the initial Random Forest $F_{(0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Iteration $i$:\n",
    "    - Define the proximity to example $\\x^\\ip$\n",
    "        - $\\proxtx{\\x^\\ip} = \\text{# of trees in } F_{(i-1)} \\text{ with } \\tx, \\x^\\ip \\text{ in same leaf}$\n",
    "    - Update imputed value $\\txi_f$\n",
    "$$\n",
    "\\txi_f = \\sum_{i=1}^m { \\proxtx{\\x^\\ip} \\x^\\ip_f }\n",
    "$$\n",
    "    - Create next Random Forest $F_{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Iterate until convergence.\n",
    "\n",
    "The authors suggest 4-6 iterations suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Missing feature in Test example\n",
    "\n",
    "Method  similar to that for a missing feature in a Training example, once\n",
    "we deal with a crucial difference\n",
    "- there is **no label** for the test example (that's what we're trying to predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose the classification target $\\y \\in C$ (i.e., the possible labels)\n",
    "$$C = \\{ c_k | 1 \\le k \\le |C| \\}$$\n",
    "\n",
    "- For each $c \\in C$:\n",
    "    - $\\txi_{f,c}$ is the imputed value obtained from the above by assuming $\\y^{(m')} = c$\n",
    "        - that is, run the missing feature for training algorithm assuming label of $\\tx$ is $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now have one imputed value $\\txi_{f,c}$  and final Random Forest per class $c$.\n",
    "\n",
    "Which one do we choose ?\n",
    "\n",
    "Observe that the $c^{th}$ Random Forest *should* predict class $c$ given\n",
    "input $\\tx$ since we set $\\y^{(m')} = c$\n",
    "\n",
    "So we choose the forest and imputed value $\\txi_{f,c}$ \n",
    "- from the class $c$\n",
    "in which $\\tx$ is most often classified as being in class $c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now casting\n",
    "\n",
    "The field of economic forecasting encounters a problem similar to missing data\n",
    "- many economic indices are combinations of sub-indices\n",
    "    - sub-indices published at different frequencies\n",
    "    - sub-indices published on different days\n",
    "    \n",
    "The \"total\" index can't be computed until **all** sub-indices have been released.\n",
    "\n",
    "So with respect to an \"early\" publication date, some sub-index data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now-casting (a play on \"forecasting\") uses techniques to make early predictions of sub-index values\n",
    "\n",
    "In some cases they use features $\\z$ believed to be correlated with actual features $\\x$:\n",
    "- derive higher frequency values for low frequency data (annual GDP, monthly Manufacturing)\n",
    "    - National Manufacturing employment may be highly correlated to Employment in a few states\n",
    "     - state-level employment may be published\n",
    "         - at higher frequency\n",
    "         - earlier date\n",
    "  - Many of these low frequency features are *composites* of other features\n",
    "     - some elements of the composite are released before others\n",
    "         - may predict the whole\n",
    "\n",
    "   - [Now-casting site](https://www.now-casting.com/home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Missing data imputation in `sklearn`\n",
    "- [`SimpleImputer`, `IterativeImputer`](https://scikit-learn.org/stable/modules/impute.html)\n",
    "- [`IterativeImputer` with different regression estimators](https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
