{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "%\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the RNN: update equations\n",
    "\n",
    "An RNN layer, at time step $\\tt$\n",
    "- Takes input element $\\x_\\tp$\n",
    "- Updates latent state $\\h_\\tp$\n",
    "- Optionally outputs $\\y_\\tp$\n",
    "\n",
    "according to the equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \\\\\n",
    "\\y_\\tp & = &  \\W_{hy} \\h_\\tp  + \\b_y \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where \n",
    "- $\\phi$ is an activation function (usually $\\tanh$)\n",
    "- $\\W$ are the weights of the RNN layer\n",
    "    - partitioned into $\\W_{xh}, \\W_{hh}, \\W_{hy}$\n",
    "    - $\\W_{xh}$: weights that update $\\h_\\tp$ based on $\\x_\\tp$\n",
    "    - $\\W_{hh}$: weights that update $\\h_\\tp$ based on $\\h_{(\\tt-1)}$\n",
    "    - $\\W_{hy}$: weights that update $\\y_\\tp$ based on $\\h_\\tp$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notes**\n",
    "- The RNN literature uses $\\phi$ rather than $a_\\llp$ to denote an activation function\n",
    "- This is the update equation for a single example $\\x^\\ip$\n",
    "- In practice, we can simultaneously update for *multiple examples*\n",
    "    - The $m' \\lt m$ examples in a minibatch, as examples are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try to understand these equations\n",
    "$$\n",
    "\\h_\\tp  =  \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \n",
    "$$\n",
    "\n",
    "$\\h_\\tp$ is the latent state after time step $\\tt$\n",
    "- It is a *vector* of length $|| \\h ||$\n",
    "- We drop the time subscript as the dimension on each step is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\W_{xh}\\x_\\tp$ must therefore *also be a vector* of length $|| \\h ||$\n",
    "- $|| \\W_{xh} ||$ is a matrix of shape $( || \\h || \\times || \\x ||)$\n",
    "- $\\h_j$, the $j^{th}$ element of latent state $\\h$ is the dot product of row $j$ of  $\\W_{xh}$ and $\\x$\n",
    "- So $\\W_{xh}^{(j)}$ describes how input $\\x_\\tp$ influences new state $h_{\\tp,j}$\n",
    "\n",
    "That is: there are separate weights for each $j$ that describe the interaction of $\\h$ and $\\x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, $\\W_{hh}\\h_{(t-1)}$ must be a *vector* of length $|| \\h ||$\n",
    "- $|| \\W_{hh} || $ is a matrix of shape $( || \\h || \\times || \\h || )$\n",
    "- So $\\W_{hh}^{(j)}$ describes how prior state $\\h_{(\\tt-1)}$ influences new state $\\h_{\\tp,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\b_h$, the bias/threshold must also be a vector of length $|| \\h ||$\n",
    "- It adjusts the threshold of activation function $\\phi$\n",
    "- As per our practice: we will usually fold $\\b$ into the weight matrices $\\W_{xh}, \\W_{hh}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, activation $\\phi$ maps a vector of length $|| \\h ||$ to another vector of length $|| \\h ||$ \n",
    "- The updated state\n",
    "\n",
    "So updated latent state $\\h_\\tp$ is influenced\n",
    "- By the input $\\x_\\tp$\n",
    "- The prior latent state $\\h_{(\\tt-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second equation\n",
    "$$\\y_\\tp  =   \\W_{hy} \\h_\\tp  + \\b_y$$\n",
    "\n",
    "is just a \"translation\" of the latent state $\\h_\\tp$ \n",
    "- To $\\y_\\tp$, the $\\tt^{th}$ element of the output sequence\n",
    "- $|| \\W_{hy} || $ is a matrix of shape $( || \\y || \\times || \\h || )$\n",
    "    - $|| \\y ||$ is the length of each output element and is problem dependent\n",
    "    - For example: a OHE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is common to equate $\\y_\\tp = \\h_\\tp$\n",
    "- No separate \"output\"\n",
    "- Just the latent state\n",
    "- Particularly when using stacked RNN layers\n",
    "    - $\\y_\\tp$ becomes the input to the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Equation in pseudo-matrix form\n",
    "\n",
    "You will often see a short-hand form of the equation.\n",
    "\n",
    "Look at $\\h_\\tp$ as a function of two inputs $\\x_, \\h_{(t-1)}$.\n",
    "\n",
    "We can stack the two inputs into a single matrix.\n",
    "\n",
    "Stack the two matrices $\\W_{xh}, \\W_{hh}$ into a single weight matrix\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp  = \\W \\mathbf{I} + \\b \\\\\n",
    "\\text{ with } \\\\\n",
    "\\W = \\left[\n",
    " \\begin{matrix}\n",
    "    \\W_{xh} & \\W_{hh}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\mathbf{I} = \\left[\n",
    " \\begin{matrix}\n",
    "    \\x_\\tp  \\\\\n",
    "    \\h_{(t-1)}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stacked RNN layers revisited\n",
    "\n",
    "With the benefit of the RNN update equations, we can clarify how stack RNN layers works.\n",
    "\n",
    "Let superscript $[\\ll]$ denote a stacked layer of RNN.\n",
    "\n",
    "So the RNN update equation for the bottom layer $1$ becomes\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h^{[1]}_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h^{[1]}_{(t-1)}  + \\b_h) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The RNN update equation for layer $[\\ll]$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h^{[\\ll]}_\\tp & = & \\phi(\\W_{xh}\\h^{[\\ll-1]}_\\tp  + \\W_{hh}\\h^{[\\ll]}_{(t-1)}  + \\b_h) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is: the input to layer $[\\ll]$ is $\\h^{[\\ll-1]}_\\tp$ rather than $\\x_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "As usual, the objective of training is to find the weights $\\W$ that minimize a loss function \n",
    "\n",
    "$$\\loss = L(\\hat{\\y},\\y; \\W)$$\n",
    "which is the average of per example losses $\\loss^\\ip$\n",
    "$$\\loss = \\frac{1}{m} \\sum_{i=1}^m { \\loss^\\ip }$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the output is a sequence\n",
    "- It's important to recognize that the *target* is a sequence too !\n",
    "- So the per example loss has an added temporal dimension\n",
    "- Loss per example *per time step*\n",
    "- Comparing the *predicted* $\\tt^{th}$ output $\\hat{\\y}^\\ip_\\tp$ to the $\\tt^{th}$ target $\\y^\\ip_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the case that the API outputs sequences\n",
    "- $\\loss^\\ip = \\sum_{\\tt=1}^T \\loss^\\ip_\\tp$\n",
    "\n",
    "In the case that the API outputs a single value\n",
    "- $\\loss^\\ip = \\loss_{(T)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss: Forward pass</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss.png\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
