{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import training_models_helper as tmh\n",
    "%aimport training_models_helper\n",
    "\n",
    "tm = tmh.TrainingModelsHelper()\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()\n",
    "\n",
    "kn = tmh.KNN_Helper()\n",
    "\n",
    "import transform_helper\n",
    "%aimport transform_helper\n",
    "\n",
    "th = transform_helper.Transformation_Helper()\n",
    "\n",
    "iph = transform_helper.InfluentialPoints_Helper()\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding the Loss function\n",
    "\n",
    "In performing Error Analysis out of sample, we *identified* **test** examples where our model failed to generalize.\n",
    "\n",
    "What can we do to make the model better ?  How can we influence the models' choice of $\\Theta$ to lead\n",
    "to a better fit ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the mapping of probability to prediction\n",
    "\n",
    "$$\n",
    "\\hat{\\y}^\\ip = \n",
    "\\left\\{\n",
    "    {\n",
    "    \\begin{array}{lll}\n",
    "     \\text{Negative} & \\textrm{if } \\hat{\\mathbf{p}}^\\ip   < 0.5  \\\\\n",
    "     \\text{Positive}& \\textrm{if } \\hat{\\mathbf{p}}^\\ip \\ge 0.5 \n",
    "    \\end{array}\n",
    "    }\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where, for Logistic Regression,  probability $\\hat{\\mathbf{p}}^\\ip$ is a function of $\\x^\\ip$ and parameters $\\Theta$.\n",
    "$$\n",
    "\\hat{\\mathbf{p}}^\\ip = \\sigma( \\Theta^T \\cdot \\x^\\ip )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we observed [before](Classification_Loss_Function.ipynb#Classification:-Loss-function),\n",
    "- The prediction for example $i$ changes only when probability $\\hat{\\mathbf{p}}^\\ip$ crosses the threshold.\n",
    "- Thus: the Performance Metric of Accuracy *won't vary continuously with changes in* $\\Theta$\n",
    "- The **Loss Function** (Cross Entropy) **will** vary continuously with changes in $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, in order to influence prediction through parameters $\\Theta$\n",
    "- We need to focus on the Loss Function\n",
    "- Which uses **in sample** (Training) examples, rather than out of sample (Test) examples\n",
    "- In the hope that better in sample performance (lower Loss) leads to better out of sample performance\n",
    "(higher Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A set of parameter values $\\Theta$ which pushes $\\hat{\\mathbf{p}}^\\ip$ in the right direction\n",
    "- Increasing $\\hat{\\mathbf{p}}^\\ip$ for Positive examples $i$\n",
    "- Increasing  $(1-\\hat{\\mathbf{p}}^\\ip)$ for Negative examples $i$\n",
    "\n",
    "is preferred to the set of parameter values $\\Theta'$ which pushes the predicted probability $\\hat{\\mathbf{p}}^\\ip$\n",
    "in the wrong direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Loss Analysis, we will be applying similar techniques as in Error Analysis\n",
    "- But using training examples rather than test examples\n",
    "- Trying to reduce the Loss Function rather than increase the Performance Metric\n",
    "\n",
    "under the assumption that better in sample performance will lead to better out of sample performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the basics of minimizing Loss Functions\n",
    "- Predictions $h(\\x; \\Theta)$  are a function of both inputs and parameters $\\Theta$\n",
    "- A given $\\Theta$ induces a per-example loss $\\loss_\\Theta^\\ip$\n",
    "- Average Loss is the average of the per-examples losses $\\loss_\\Theta^\\ip, i=1, \\ldots, m$\n",
    "- We seek the optimal $\\Theta^*$: $$\n",
    "\\Theta^* = \\argmin{\\Theta} { \\loss_\\Theta }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In pictures:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/W1_L4_s55_Intro_training.png\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis.png\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "â€¢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional loss\n",
    "\n",
    "In Error Analysis we partition test examples into groups with some common property, such as\n",
    "- Commonality of result: TP, FN, TN, FP\n",
    "- Commonality of features\n",
    "in order to compute a *conditional* out of sample metric.\n",
    "\n",
    "In Loss Analysis we partition training examples into groups to\n",
    "in order to compute a *conditional* in sample metric.\n",
    "\n",
    "The following picture uses colors to identify which group a training example belongs to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Loss analysis: conditional loss</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis_1.png\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The real advantage of performing Conditional analysis in sample\n",
    "- In sample examples (training/validation) can be re-used, unlike Test examples\n",
    "- Added features based on in sample analysis is likely to affect the Loss\n",
    "    - Unknown whether it will affect Performance Metric (when it is different than Loss, e.g., Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What can we do to reduce loss ?\n",
    "\n",
    "Understanding the per example loss can help you \"push\" the optimizer toward find a \"better\" $\\Theta$.\n",
    "\n",
    "We will outline some simple strategies via examples that identify a probelm and propose a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Increase number of \"problem\" training example\n",
    "\n",
    "For MNIST digit classification\n",
    "- We hypothesize a commonality that causes images of the digit 8 to be mis-classified\n",
    "    - 8's that are slanted in the \"opposite\" direction of normal\n",
    "    - We will refer to this as the *problematic* class\n",
    "\n",
    "One reason our classifier may fail on this sub-class of 8's\n",
    "- There are many fewer of them than the more prevalent images of 8's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mathematically, the Average Loss is equally weighted\n",
    "$$\n",
    "\\loss_\\Theta  = { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "$$\n",
    "\n",
    "but the cumulative weight of the problematic class (mis-shaped 8's) is very small.\n",
    "\n",
    "So even if all examples in the problematic class were mis-classified\n",
    "- The impact on Average Loss may be sufficiently small.\n",
    "- That $\\Theta$ doesn't get updated in the direction that will improve these examples\n",
    "    - Especially if we end optimization before absolute convergence occurs, as is common\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One strategy for pushing the model to better fit the problematic examples is\n",
    "- Increase their cumulative weight in the Loss\n",
    "- By increasing their number !\n",
    "\n",
    "The strategy known as *Data Augmentation* adds examples to the Training examples\n",
    "- Here we try to find/synthesize more instances of the problematic type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can augment examples by repeating them (as above).\n",
    "\n",
    "Another method for augmentation is to perturb the example's feature vector in a way that preserves the example's label.\n",
    "\n",
    "For example, we may\n",
    "- add some noise to the feature vector\n",
    "- Perform data-type specific transformations\n",
    "    - Images: [shift, rotate, transpose](DataAugmentation/Data_augmentation.ipynb#Original-image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Influential points\n",
    "\n",
    "We have described the case where the issue is mis-classification of an important but small sub-class.\n",
    "- Which results in a small cumulative contribution to the Loss \n",
    "\n",
    "Sometimes the problem is a small sub-class with an *out-sized* contribution to the Loss\n",
    "- Having a few problem examples\n",
    "- Whose contribution to Loss is so large\n",
    "- That it pushes $\\Theta$ in the wrong direction for the more numerous non-problem examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is: $\\loss_\\Theta^\\ip$ is so large (for some example $i$) that \n",
    "- $\\Theta$ is changed to reduce $\\loss_\\Theta^\\ip$ \n",
    "- Resulting in an increase in $\\loss_\\Theta^{(i')}$ for each non-problematic example $i'$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The phenomenon we just described is sometimes called *Influential Points*.\n",
    "\n",
    "These have been particularly well-studied in the context of Linear Regression.\n",
    "\n",
    "We will use Linear Regression as an illustration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Loosely speaking, an example is **influential** if \n",
    "- the parameter estimate $\\Theta$ changes greatly depending on whether the example is included/excluded\n",
    "\n",
    "Feature values on the extreme ends of the range have greater potential\n",
    "for being influential.\n",
    "\n",
    "This is one argument for constraining the range of the feature (MinMax, Standardization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's an interactive tool to get a feel for influential points in Linear Regression.\n",
    "\n",
    "It allows you to change the value of a single data point and see the effect on the fitted line.\n",
    "\n",
    "Observe how the slope changes (displayed in the title)\n",
    "- 10 labeled examples $\\{ [\\x^\\ip, \\y^\\ip] \\, | \\, 0 \\le i \\lt 10  \\}$\n",
    "- The top slider chooses the index $i \\in  \\{ 0 \\ldots 9 \\}$ of one data point to change\n",
    "- The bottom slider is the new value $\\y^\\ip$ for the point at the chosen index $i$\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate some points\n",
    "(x_ip,y_ip) = iph.gen_data(10)\n",
    "\n",
    "# Fit a line to the points; get a function to update the fit and the plot\n",
    "fit_update = iph.plot_init()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d942490bba8463a9131a3b3b8135cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='Index of pt:', max=9), IntSlider(value=0, description='Nâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iph.plot_interact(fit_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Play around with the tool\n",
    "- Choose an index near the middle (set top slider to a value of around 5)\n",
    "    - Move the bottom slider, observing the effect on the fitted line (slope displayed in the title)\n",
    "- Choose an index at either extreme (index 0 or 9)\n",
    "    - Move the bottom slider, observing the effect on the fitted line (slope displayed in the title)\n",
    "\n",
    "Observe \n",
    "- Changing $\\y^\\ip$ for $i$ near the middle of the range has little effect on the fit\n",
    "- Changing $\\y^\\ip$ for $i$ near either end ($0$ or $9$) has a large effect on the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The solution is to somehow reduce example $i$'s contribution to Average Loss\n",
    "- Removing the example: possible data error or outlier\n",
    "- Down-weighting\n",
    "- Clipping the values of the features/target to some upper bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Further background \n",
    "\n",
    "Consider feature $j$.\n",
    "\n",
    "The **leverage** of example $i$ is related to\n",
    "- How far $\\x_j^\\ip$ is from $\\bar{\\x}$, the average of $\\x_j$ across all examples\n",
    "\n",
    "It is not always the case, but high leverage sometimes makes the point influential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reference:\n",
    "[Influence from leverage and distance](http://onlinestatbook.com/2/regression/influential.html)\n",
    ">An observation's influence is a function of two factors: (1) how much the observation's value on the predictor variable differs from the mean of the predictor variable and (2) the difference between the predicted score for the observation and its actual score. The former factor is called the observation's leverage. The latter factor is called the observation's distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Calculation of Leverage (h) of example $i$, feature $j$\n",
    "\n",
    "[formula](https://learnche.org/pid/least-squares-modelling/outliers-discrepancy-leverage-and-influence-of-the-observations#leverage)\n",
    "\n",
    "$$ \n",
    "\\begin{array}{lll}\n",
    "h^\\ip_j & = & { 1 \\over n }+ \\frac{ (\\x^\\ip_j - \\bar{\\x_j})^2}{ \\sum_i { (\\x^\\ip_j - \\bar{\\x_j})^2} } \\\\\n",
    "    & = & \\frac{ 1 + \\left( \\frac{\\x^\\ip_j - \\bar{\\x_j}}{\\sigma_{\\x_j} } \\right) ^2}{n}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You can see that the leverage of $\\x^\\ip_j$ depends on the (standardized) distance of $\\x^\\ip_j$ from the mean (over all $i$) of $\\x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
