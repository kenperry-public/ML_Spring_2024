{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\n",
    "\\newcommand{\\likeli}{\\mathbb{L}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "Our treatment of Loss functions thus far has been somewhat superficial.\n",
    "\n",
    "It's time to dive deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To review\n",
    "- The per example Loss is a measure of the success of a prediction on a **training** example\n",
    "- Predictions are a function of parameters $\\Theta$\n",
    "- \"Fitting\" or \"training\" a model is the process of find the best $\\Theta$\n",
    "    - The optimal $\\Theta$ is the one that minimizes average (across training examples) Loss\n",
    "    \n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss^\\ip_\\Theta & =  & L( \\;  h(\\x^\\ip; \\Theta),  \\y^\\ip \\;) = L( \\hat{\\y}^\\ip , \\y)  \\\\\n",
    "\\loss_\\Theta  & = & { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta \\\\ \\\\\n",
    "\\Theta^* = \\argmin{\\Theta} { \\loss_\\Theta }\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The purpose of this module is to present a mathematical basis behind\n",
    "some of the common Loss functions in  Machine Learning\n",
    "- Mean Squared Error (MSE), used in Regression task\n",
    "- Cross Entropy, used in Classification task\n",
    "- Kullback Leibler (KL) Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Likelihood\n",
    "\n",
    "The statistical method known as Likelihood Maximization is the fundamental tool we will use.\n",
    "\n",
    "Let us conceptualize our set of training examples\n",
    "$$\\langle \\X, \\y \\rangle= [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ]$$\n",
    "\n",
    "as being *samples* from an unknown distribution \n",
    "$$\\pdata(\\y \\; | \\; \\x)$$\n",
    "which we call the *true* or *actual*  distribution\n",
    "- *Conditional* distribution:  target, conditional on feature\n",
    "\n",
    "The distribution of the sample $\\langle \\X, \\y \\rangle$ is called the *empirical* distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given the actual distribution $\\pdata(\\x, \\y)$,\n",
    "- The likelihood (i.e, probability) of drawing the $m$ particular examples in the training data\n",
    "- Assuming independence, is\n",
    "\n",
    "$$\n",
    "\\likeli_{\\text{data}} = \\prod_{i=1}^m { \\pdata(\\y^\\ip \\; | \\; \\x^\\ip) }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By taking the logarithm, we turn this product into a sum\n",
    "\n",
    "$$\n",
    "\\log(\\likeli_{\\text{data}}) = \\sum_{i=1}^m { \\log \\left( \\pdata(y^\\ip \\; | \\; \\x^\\ip) \\right) }\n",
    "$$\n",
    "\n",
    "called the *Log Likelihood* of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the true process that generates examples is unknown to us; all we have is a sample (the empirical distribution) from the actual distribution.\n",
    "\n",
    "We can *hypothesize* the existence of some process that generates the training data\n",
    "- A Linear Model generates examples $\\hat{\\y}^\\ip = \\Theta^T \\cdot \\x^\\ip$\n",
    "- There may be more complex models we can suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But, given our limitations, our hypothetical model does not exactly match our examples\n",
    "$$\n",
    "\\y^\\ip = \\hat{\\y}^\\ip + \\epsilon^\\ip\n",
    "$$\n",
    "\n",
    "where $\\epsilon^\\ip$ is the error between our hypothesized $\\y^\\ip$ and the one we observe.\n",
    "\n",
    "\n",
    "This means that the conditional distribution of targets is\n",
    "centered around the model (predicted) value $\\hat{\\y}^\\ip$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our limitations\n",
    "- Maybe we can only *measure* $\\y^\\ip$ with error\n",
    "- There is a missing feature that caused the error  \n",
    "    - Had this feature been included, there would be no error\n",
    "        - Example: from everything I know about you, I observe that you *never* buy coffee at night\n",
    "        - One night you buy coffee -- to bring to a friend, which is not a feature we capture\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Putting this all together\n",
    "- The observed error is defined as\n",
    "$$\n",
    "\\epsilon^\\ip = \\y^\\ip - \\hat{\\y}^\\ip\n",
    "$$\n",
    "\n",
    "- A linear hypothesis for the true distribution is\n",
    "$$\n",
    "\\hat{\\y} = \\Theta^T \\cdot \\x\n",
    "$$\n",
    "\n",
    "- The observed targets differ from our hypothesis by the observed error\n",
    "\n",
    "\n",
    "$$\n",
    "\\y^\\ip = \\Theta^T \\cdot \\x^\\ip + \\epsilon^\\ip\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Do these equations look familiar ?\n",
    "- These are exactly the equations for Linear Regression\n",
    "- Only the story we told is different\n",
    "    - We didn't start with the goal of approximating $\\y$\n",
    "    - We adopt the standpoint that $\\y$ differs from the hypothesized $\\hat{\\y}$ because of some error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our hypothesis (now referred to as the *model*) is parameterized by $\\Theta$.\n",
    "\n",
    "It implies a conditional distribution of targets\n",
    "$$\\pmodel(\\y \\; | \\; \\x; \\Theta)$$\n",
    "called the *model* or *predicted* distribution\n",
    "\n",
    "Predicted distribution $\\pmodel(\\y \\; | \\; \\x ;\\Theta)$ is an *approximation* of actual distribution $\\pdata( \\y \\; | \\; \\x )$.\n",
    "\n",
    "We now refer to the model values $\\hat{\\y}^\\ip$ as *predictions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We introduce the statistical concept known as Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We will subsequently relate our Loss functions to MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose the true process $\\pdata(\\x, \\y)$ is\n",
    "$$\\y^\\ip = 2 * \\x^\\ip$$\n",
    "\n",
    "If our model $$\\pmodel(\\y \\; | \\; \\x ;\\Theta)$$ is\n",
    "$$\\y^\\ip = 1 + 3 * \\x^\\ip + \\epsilon^\\ip$$\n",
    "\n",
    "then the errors $\\epsilon^\\ip$ are systematically incorrect.\n",
    "- Mean error won't be 0\n",
    "- $\\sigma$, the standard deviation of errors, won't be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What this means is that\n",
    "- Under the model's poor assumptions for $\\Theta$\n",
    "- It is *less likely* for us to draw the $m$ particular examples in the training data\n",
    "    - Compared to an assumption for $\\Theta$ that is closer to the actual\n",
    "\n",
    "The likelihood under the model is written\n",
    "$$\n",
    "\\likeli_{\\text{model}} = \\prod_{i=1}^m { \\pmodel(\\y^\\ip \\; | \\; \\x; \\Theta) }\n",
    "$$\n",
    "\n",
    "- With a poorly chose $\\Theta$, errors are large, and the likelihood is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Under this framework, the best model\n",
    "- Is the choice of $\\Theta$\n",
    "- That maximizes the likelihood of drawing the $m$ particular examples in the training data\n",
    "\n",
    "This is called *Maximum Likelihood Estimation*\n",
    "\n",
    "$$\n",
    "\\Theta^* = \\argmax{\\Theta}{\\sum_{i=1}^m { \\log(\\pmodel(\\y^\\ip \\; | \\; \\x^\\ip; \\Theta)) } }\n",
    "$$\n",
    "\n",
    "(Note that maximizing the *log* likelihood is the same as maximizing the likelihood).\n",
    "\n",
    "[Deep Learning Book 5.5](https://www.deeplearningbook.org/contents/ml.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss functions for Machine Learning\n",
    "\n",
    "We now show that our choice of Loss functions\n",
    "- MSE for Regression\n",
    "- Cross Entropy for Classification\n",
    "\n",
    "can be justified in terms of\n",
    "**maximization of the log likelihood**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression: Log Likelihood of Linear models with normal errors\n",
    "\n",
    "Under the hypothesis of a Linear Model, we have\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y^\\ip & = & \\hat{\\y}^\\ip + \\epsilon^\\ip \\\\\n",
    "\\hat{\\y}^\\ip & = & \\Theta^T \\cdot \\x^\\ip \\\\\n",
    "\\epsilon^\\ip & = & \\y^\\ip - \\hat{\\y}^\\ip\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We had not previously made any assumption about the nature of $\\epsilon^\\ip$\n",
    "\n",
    "Suppose it is normally distributed \n",
    "$$\\epsilon^\\ip = \\mathcal{N}(0,\\sigma)$$\n",
    "\n",
    "This means $\\prc{\\y^\\ip}{\\x^\\ip ; \\Theta}$ is $\\mathcal{N}(\\hat{\\y}^\\ip,\\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Substituting the formula for Normal distribution, the conditional probability of $\\y^\\ip$ given $\\x^\\ip$ is\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\prc{\\y^\\ip}{\\x^\\ip ; \\Theta} & = & \\frac{1}{\\sigma \\sqrt(2\\pi)} \\exp(- \\frac{(\\y^\\ip - \\hat{\\y}^\\ip)^2}{2\\sigma}) &   \\prc{\\y^\\ip}{\\x^\\ip ; \\Theta} \\text{ is }\\mathcal{N}(\\hat{\\y}^\\ip,\\sigma); \\\\ & & & \\text{def. of Normal} \\\\\n",
    "& = & \\frac{1}{\\sigma \\sqrt(2\\pi)} \\exp(- \\frac{(\\epsilon^\\ip)^2}{2\\sigma}) &  \\epsilon^\\ip = \\y^\\ip - \\hat{\\y}^\\ip\\\\\n",
    "    & \\propto &\\exp(- \\frac{(\\epsilon^\\ip)^2}{2 \\sigma})  \\\\  \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Likelihood of the training set, given this model of the conditional probability,\n",
    "is just the product over the training set of $\\pr{\\y^\\ip | \\x^\\ip}$:\n",
    "$$\n",
    "\\mathbb{L}_{\\text{model}} = \\prod_{i=1}^m { \\prc{\\y^\\ip}{\\x^\\ip ; \\Theta} }\n",
    "$$\n",
    "and the Log Likelihood is\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\mathbb{l}_{\\text{model}} & = & \\log \\left( \\prod_{i=1}^m { \\prc{\\y^\\ip}{\\x^\\ip ; \\Theta} } \\right) \\\\\n",
    "& = &  \\sum_{i=1}^m { \\log \\left( \\prc{\\y^\\ip}{\\x^\\ip ; \\Theta} \\right) } \\\\\n",
    "& \\propto &  \\sum_{i=1}^m { \\log \\left( \\exp(- \\frac{(\\epsilon^\\ip)^2}{2 \\sigma}) \\right) } \\\\\n",
    "& = &  \\sum_{i=1}^m { - \\frac{(\\epsilon^\\ip)^2}{2 \\sigma}} \\\\\n",
    "& =  &   - \\frac{1}{2 \\sigma} \\sum_{i=1}^m { {(\\y^\\ip - \\Theta^T \\cdot \\x^\\ip)^2}} \\\\\n",
    "& \\propto & - \\sum_{i=1}^m { {(\\y^\\ip - \\Theta^T \\cdot \\x^\\ip)^2}} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You should recognize the (negative of) the Log Likelihood as the Mean Squared Error (MSE).\n",
    "\n",
    "Thus minimizing MSE Loss function,  which we originally presented without justification\n",
    "- Is equivalent to finding the model that maximizes the likelihood of the actual distribution \n",
    "\n",
    "Stated another way\n",
    "- The MSE Loss\n",
    "- Gives rise to the $\\Theta$ obtained by MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification: Log Likelihood of Binary classification\n",
    "\n",
    "Review of Binary Classification:\n",
    "- We encode the Positive labels $\\y^\\ip$ with the number 1 and Negative labels with the number 0\n",
    "- For example $i$ we compute $\\hat{p}^\\ip = \\pr{\\y^\\ip = \\text{Positive} \\; | \\; \\x^\\ip}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The conditional probability for a target $\\y^\\ip$ given the features $\\x^\\ip$ can be written as\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\prc{\\hat{\\y}^\\ip}{\\x^\\ip; \\Theta} & = & \\prc{\\y^\\ip = \\text{Positive}}{\\x^\\ip; \\Theta} \n",
    "& + & \\prc{\\y^\\ip = \\text{Negative}}{\\x^\\ip; \\Theta} & \\text{definition} \\\\\n",
    "& = & \\prc{\\y^\\ip = \\text{Positive}}{\\x^\\ip; \\Theta}^{\\y^\\ip} \n",
    "& * & \\prc{y^\\ip = \\text{Negative}}{\\x^\\ip; \\Theta}^{(1 - \\y^\\ip)} & \\text{One of } \\y^\\ip, (1-\\y^\\ip) \\text{ is } 0, \\text{ other is } 1 \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Substituting the above probability into the \n",
    "Likelihood\n",
    "$$\n",
    "\\likeli_{\\text{model}} = \\prod_{i=1}^m { \\pmodel(\\y^\\ip \\; | \\; \\x^\\ip; \\Theta) }\n",
    "$$\n",
    "\n",
    "and taking the log\n",
    "$$\n",
    "\\begin{array}[lllll]\\\\\n",
    "\\mathcal{l} & = & \\log(\\likeli_{\\text{model}}) \\\\\n",
    "\\mathcal{l} & = & \\sum_{i=1}^m { \\y^\\ip * \\log \\left( \\prc{\\y^\\ip = \\text{Positive}}{\\x^\\ip; \\Theta}\\right) \n",
    "+ (1-\\y^\\ip ) * \\log \\left( \\prc{\\y^\\ip = \\text{Negative}}{\\x^\\ip; \\Theta}\\right)} \\\\\n",
    "& = & \\sum_{i=1}^m { \\y^\\ip * \\log(\\hat{p}^\\ip)  + (1-\\y^\\ip ) * \\log( 1 - \\hat{p}^\\ip )} \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recalling  the per-example Loss function for Binary Classification\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss^\\ip_\\Theta & = & - \\left( \\y^\\ip*\\log(\\hat{p}^\\ip) + (1-\\y^\\ip) * \\log(1-\\hat{p}^\\ip) \\right) \\\\\n",
    "\\loss_\\Theta  & = &{ 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You see that $\\frac{1}{m}$ times the negative of the Log Likelihood is equal to the Binary Cross Entropy Loss.\n",
    "\n",
    "Thus minimizing Binary Cross Entropy loss,  which we originally presented without justification\n",
    "- Is equivalent to finding the model that maximizes the likelihood of the actual distribution \n",
    "\n",
    "Stated another way\n",
    "- The Binary Cross Entropy Loss\n",
    "- Gives rise to the $\\Theta$ obtained by MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KL divergence\n",
    "\n",
    "We can now motivate the KL divergence:\n",
    "- The difference between the log likelihood\n",
    "of the empirical and model distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bayes Theorem relates joint and conditional probabilities\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\prc{\\y}{\\x } & = & \\frac{\\pr{\\x,\\y}} {\\pr{\\x}} \\\\\n",
    "\\pr{\\x,\\y} & = & \\prc{\\y}{\\x} \\; \\pr{\\x}  & \\text{re-arrange the terms} \\\\\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So we can re-write\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\log(\\likeli_{\\text{model}}) & = & \\sum_{i=1}^m { \\log \\left( \\pmodel(\\x^\\ip, \\y^\\ip; \\Theta) \\right) } \\\\\n",
    "                             & = & \\sum_{i=1}^m { \\log \\left( \\pmodel(\\y^\\ip | \\x^\\ip ; \\Theta) \\right) \\; \\pr{\\x^\\ip} } \\\\\n",
    "                             & = & \\E_{\\x \\sim \\pdata}  {\\log \\left( \\pmodel(\\y^\\ip | \\x^\\ip ; \\Theta) \\right)} \n",
    "\\end{array}\n",
    "$$\n",
    "and similarly for $\\log(\\likeli_{\\text{data}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The difference between the log likelihoods of the two distributions \n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\log(\\likeli_{\\text{data}}) - \\log(\\likeli_{\\text{model}}) & = &\n",
    "    \\E_{\\x \\sim \\pdata} { \\left( \\log(\\pdata(\\y | \\x))  \\right) }  - \\E_{\\x \\sim \\pdata} { \\left( \\log(\\pmodel(\\y | \\x; \\Theta)) \\right)} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The above difference is called the *KL Divergence* between the distributions.\n",
    "\n",
    "It is a measure of the \"closeness\" of two distributions.\n",
    "\n",
    "This means\n",
    "- Minimizing KL Divergence between the actual and predicted distributions\n",
    "- Is equivalent to minimizing the difference between the log likelihoods of the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The optimal $\\pmodel(\\y | \\x; \\Theta))$ is the one with smallest KL Divergence\n",
    "\n",
    "Since only $\\pmodel(\\y | \\x; \\Theta))$ is a function of $\\Theta$, the $\\Theta$ that minimizes KL Divergence\n",
    "is found by minimizing\n",
    "$$\n",
    " - \\E_{\\x \\sim \\pdata} { \\left( \\log(\\pmodel(\\y \\; | \\; \\x; \\Theta)) \\right) }\n",
    " $$\n",
    "which is expression for the Cross Entropy Loss.\n",
    "\n",
    "Thus minimizing Cross Entropy Loss,  which we originally presented without justification\n",
    "- Is equivalent to minimizing the KL Divergence between the actual and predicted distributions\n",
    "- Is equivalent to minimizing the difference between the log likelihoods of the distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Although we have not yet covered Unsupervised Learning, we observe that\n",
    "Likelihood Maximization can be applied there as well\n",
    "- Unsupervised Learning has no targets\n",
    "- So training examples \n",
    "$$\\langle \\X \\rangle= [ \\x^\\ip | 1 \\le i \\le m ]$$\n",
    "- The distribution is over features, not of targets conditional on features\n",
    "$$\\pdata(\\x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss functions for Deep Learning: Preview\n",
    "\n",
    "The Loss functions for Classical Machine Learning were perhaps motivated by the desire for closed form solutions.\n",
    "\n",
    "In Deep Learning, the optimization is typically solved via search.\n",
    "\n",
    "This opens the possibilities of complex loss functions that don't require closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will see in the Deep Learning part of this course, the key part of solving a task\n",
    "is in *defining* a loss function that mirrors the task's objective.\n",
    "\n",
    "Thus, many loss functions are problem specific and often quite creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cool loss functions: Neural Style Transfer\n",
    "Neural Style Transfer\n",
    "\n",
    "Given \n",
    "- a \"Content\" Image that you want to transform\n",
    "- a \"Style\" Image (e.g., Van Gogh \"Starry Night\")\n",
    "- Generate a New image that is the Content image redrawn in the style of the Style Image\n",
    "    - [Gatys: A Neural Algorithm for Style](https://arxiv.org/abs/1508.06576)\n",
    "    - [Fast Neural Style Transfer](https://github.com/jcjohnson/fast-neural-style)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Content image\n",
    "<img src=images/chicago.jpg width=500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Style image\n",
    "<img src=images/starry_night_crop.jpg width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generated image\n",
    "<img src=images/chicago_starry_night.jpg width=500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss function\n",
    "\n",
    "Definitions:\n",
    "- Style image, represented as a vector of pixels $\\vec{a}$\n",
    "- Content image, represented as a vector of pixels $\\vec{p}$\n",
    "- Generated image, represented as a vector of pixels $\\vec{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss function (which we want to minimize by varying $\\vec{x}$) has two parts\n",
    "\n",
    "$$\n",
    "\\text{L} = \\text{L}_{\\text{content}}(\\vec{p}, \\vec{x}) + \\text{L}_{\\text{style}}(\\vec{a}, \\vec{x})\n",
    "$$\n",
    "\n",
    "- a Content Loss\n",
    "    - measure of how different the generated image $\\vec{x}$ is from Content image  $\\vec{p}$\n",
    "- a Style Loss\n",
    "    - measure of how different the \"style\" of generated $\\vec{x}$ is from style of Style image $\\vec{a}$\n",
    "    \n",
    "\n",
    "Key: defining what is \"style\" and similarity of style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
