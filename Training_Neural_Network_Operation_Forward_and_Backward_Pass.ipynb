{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How a Neural Network toolkit works\n",
    "\n",
    "TensorFlow is the toolkit of primitives that underlies Keras.\n",
    "\n",
    "It is what powers training and computation in Neural Networks.\n",
    "\n",
    "Although it might seem mysterious, it (and similar toolkits) is based on a very simple concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is pseudo-code for the *training loop*\n",
    "- The part of the Keras framework that implements `fit`\n",
    "- It solves for the optimal weights $\\W^*$ that minimize the Loss function\n",
    "- Pre-Keras, the user coded this loop for each problem\n",
    "\n",
    "It is nothing more than Gradient Descent."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "initialize(W)\n",
    "\n",
    "# Training loop to implement mini-batch SGD\n",
    "for epoch in range(n_epochs):`\n",
    "    for X_batch, y_batch in next_batch(X_train, y_train, batch_size, shuffle=True):\n",
    "        # Forward pass\n",
    "        y = NN(X_batch)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = loss_fn(y, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = gradient(loss, W)\n",
    "        \n",
    "        # Update \n",
    "        W = W - grads * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We process all the training examples once per epoch\n",
    "- The epoch is divided into *mini-batches*: disjoint subsets of training examples\n",
    "- The estimate of the weights is updated in each epoch\n",
    "- We do this for many epochs, until the Loss function no longer decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each epoch consists of two phases\n",
    "- A Forward Pass in which inputs are mapped into predictions, for each example in the mini batch\n",
    "    - An Average Loss is computed over all examples in the mini batch\n",
    "- A Backward Pass in which gradients of the Average Loss are computed\n",
    "    - And used to update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Forward and Backward API\n",
    "\n",
    "There is a clever \"trick\" that facilitates\n",
    "- Computation of predictions (Forward Pass)\n",
    "- Computation of analytical derivatives (Backward Pass)\n",
    "\n",
    "<div>\n",
    "<center>Forward and Backward pass: Detail</center>\n",
    "<br>\n",
    "<img src=\"images/Backward_pass_detail.png\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Each atomic operation is implemented by an Object-Oriented Class**\n",
    "\n",
    "The class implements methods\n",
    "- `forward` for the Forward Pass\n",
    "- `backward` for the Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This trick is repeated many times, for each atomic operation.\n",
    "\n",
    "That's all there is to it: Consistent application of a simple trick !\n",
    "\n",
    "Let's illustrate using the Multiplication operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the Forward Pass\n",
    "\n",
    "The essential part of the Forward Pass is computing layer $\\\\l$'s output $\\y_\\llp$ from\n",
    "the layer's input $\\y_{(\\ll-1)}$ and the layer's weights $\\W_{(\\ll)}$.\n",
    "\n",
    "$$\n",
    "\\y_{(\\ll)} = a_{(\\ll)}( f_{(\\ll)}( \\y_{(\\ll-1)}, \\W_{(\\ll)})\n",
    "$$\n",
    "\n",
    "For simplicity of presentation, we will temporarily assume that the activation $a_\\llp$ is the identity\n",
    "function.\n",
    "\n",
    "(Without loss of generality, we can implement the activation as a separate layer that also obeys\n",
    "the per layer logic we are about to present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the atomic operation of multiplication\n",
    "`x * y`\n",
    "\n",
    "We define a class `MultiplyLayer`\n",
    "- derived from parent class `Layer`, which requires the `forward` and `backward` methods\n",
    "\n",
    "Here is the code for the Forward Pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " class MultiplyLayer(Layer):\n",
    "    \"\"\"\n",
    "    A layer that multiplies its two inputs (x,y)\n",
    "    \"\"\"\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        The forward pass: compute the product of x, y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x, y: ndarrays\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        z: ndarray that is the product of x and y\n",
    "        \"\"\"\n",
    "         "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        # Compute the product\n",
    "        z = x * y\n",
    "        \n",
    "        # Remember the two inputs: we will need to take derivatives with respect to each\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not surprisingly\n",
    "- The key statement is the one that multiplies the two inputs\n",
    "- And returns the product\n",
    "\n",
    "Just as you would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But also notice that we are saving the two multiplicands (x and y).\n",
    "\n",
    "We will need them for the Backward Pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the Backward Pass\n",
    "\n",
    "The job of the Backward Pass is\n",
    "- To take the Loss gradient $\\loss'_\\llp$ for the layer\n",
    "- Compute the Loss gradient $\\loss'_{(\\ll-1)}$ to \"flow backwards\" to the previous layer\n",
    "- Compute the Local gradients\n",
    "- Obtain the derivative with respect to $\\W_\\llp$, the layer's weights, using the Loss and Local gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the computation that takes $\\loss'_\\llp$ as input and produces $\\loss'_{(\\ll-1)}$ as output\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\loss'_{(\\ll-1)} & = & \\frac{\\partial \\loss}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And to compute the derivative of the Loss with respect to the layer's weights\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp}\n",
    "$$\n",
    "\n",
    " the Chain Rules gives us\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp} & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But note: there are **no** weights in a Multiplication layer !\n",
    "\n",
    "So we only need to compute $\\loss'_{(\\ll-1)}$ in this case.\n",
    "\n",
    "Were the operation to have weights, the code logic would be very similar to this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  def backward(self, dL_dz): \n",
    "        \"\"\"\n",
    "        This layer computes: \n",
    "           z = x * y\n",
    "        on the forward pass.\n",
    "        \n",
    "        The backward pass: \n",
    "        - Computes dL_dzz:\n",
    "          - the derivative of the loss wrt the output zz of the previous layer (which are this layer's inputs)\n",
    "          - where zz = [ x, y ]\n",
    "        - Computes dL_dW\n",
    "          - Where W are the weights of this layer (not applicable for this layer)\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        dL_dz: scalar.  \"loss gradient\": dL/dz : \n",
    "        - The derivative of the loss wrt the output (z) of this layer\n",
    "       \n",
    "        Returns\n",
    "        --------\n",
    "        [ dL_dW, dL_dzz ] where\n",
    "        - dL_dW is derivative of Loss wrt weights (not applicable for multiplication)\n",
    "        - dL_dzz =  [dL_dx, dL_dy] is derivative of Loss wrt to prior layer's outputs zz = [ x, y ] \n",
    "        \"\"\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        \"\"\"\n",
    "        Since this layer's operation  is multiplication, z = x*y\n",
    "        dz/dx = y, dz/dy = x\n",
    "        \"\"\"        \n",
    "        dz_dx = self.y, dz_dy = self.x\n",
    "         \n",
    "        # Chain rule\n",
    "        dL_dx = dL_dz * dz_dx\n",
    "        dL_dy = dL_dz * dz_dy\n",
    "        \n",
    "        dL_dzz = [ dL_dx, dL_dy ]\n",
    "        \n",
    "        # No weights W for this layer\n",
    "        dL_dW = null\n",
    "   \n",
    "        return [ dL_dW, dL_dzz ]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The `backward` method takes the loss gradient \n",
    "$\\loss'_\\llp  =  \\frac{\\partial \\loss}{\\partial \\y_\\llp}  =  \\frac{\\partial \\loss}{\\partial z}$\n",
    "\n",
    "- Computes the local gradients $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "$$\n",
    "    \\begin{array}[lll]\\\\\n",
    "    \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} & = &[ \\frac{\\partial \\y_\\llp}{\\partial x},  \\frac{\\partial \\y_\\llp}{\\partial y}] & \\text{Since } \\y_{(\\ll-1)} = [x,y]\\\\\n",
    "    & = & [ \\frac{\\partial z}{\\partial x},  \\frac{\\partial z}{\\partial y}] & \\text{Since } z = y_\\llp \\\\\n",
    "    & = & [ \\frac{\\partial (x*y)}{\\partial x},  \\frac{\\partial (x*y)}{\\partial y}] & \\text{Since } z = x*y \\\\\n",
    "    & = & [ y,  x] & \\text{Since } z = x*y \\\\\n",
    "    \\end{array}\n",
    "    $$\n",
    "    \n",
    "- Multiplies the local gradients by the loss gradient  $\\loss'_\\llp $ to get $\\loss'_{(\\ll -1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now you can see why the `forward` method stored the multiplicands `x, y`\n",
    "- They were needed as\n",
    "   $[ y, x ] = [ \\frac{\\partial (x*y)}{\\partial x},  \\frac{\\partial (x*y)}{\\partial y}] $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The whole basis of toolkits for Neural Networks is this simple Module API consisting of methods\n",
    "- `forward`\n",
    "- `backward`\n",
    "\n",
    "Knowing this: you can implement *your own* operations if you ever find that necessary.\n",
    "\n",
    "That is how more complex layers are implemented (e.g., Convolution).\n",
    "\n",
    "Hopefully this demystified the notion that Neural Network toolkits are complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
