{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()\n",
    "\n",
    "nn_ch = neural_net_helper.Charts_Helper(save_dir=\"/tmp\", visible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Create files containing charts\n",
    "create = False\n",
    "\n",
    "if create:\n",
    "    file_map = nn_ch.create_charts()\n",
    "    print(file_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap of \"Classical\" Machine Learning\n",
    "\n",
    "Canonical Machine Learning tasks\n",
    "- Classification\n",
    " - Given attributes, predict a target value from among a *discrete* set of values\n",
    "- Regression\n",
    " - Given attributes, predict a target value from a *continuous* range of values\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To solve the task \n",
    "- We create a *parameterized* model: mapping from input features to label/target\n",
    "- Fit the model parameters on \n",
    "a collection of examples (feature/target pairs) \n",
    "\n",
    "\n",
    "$$ \\langle \\X, \\y \\rangle= [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ]$$\n",
    "\n",
    "- Use the fitted model to predict a $\\hat{\\y}$ from an unseen feature vector $\\x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is often the case that we can make our model more successful (e.g. more accurate) by\n",
    "- rather than using raw inputs $\\x$\n",
    "- we transform the raw inputs \n",
    "- fit our model and make predictions on the transformed feature vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate, consider some features describing a person\n",
    "- Height\n",
    "- Weight\n",
    "- Income\n",
    "\n",
    "Our goal is to predict the target: some measure of Happiness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Extremely small differences in Height or Weight probably don't affect Happiness.\n",
    "\n",
    "We may choose to transform each variable from a continuous value to a discrete value\n",
    "- Height transformed to Height Bucket: { Short, Medium, Tall }\n",
    "- Weight transformed to Weight Bucket: { Thin, Just Right, A Little Extra }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is sometimes the case that each variable individually is a poor predictor, but the combination is powerful.\n",
    "\n",
    "In such a case, a transformation that creates a new \"combination\" feature may be helpful\n",
    "- Create a discrete binary variable Combo indicating \"Perfect Height and Perfect Weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The absolute level of Income may not be as good a predictor of Happiness as the level relative to one's peers\n",
    "- We may choose to transform Income into Relative Income: Income/(Median Income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By fitting/training on \n",
    "- (Height Bucket, Weight Bucket, Combo, Relative Income) \n",
    "- rather than (Height, Weight,Income)\n",
    "our model might make\n",
    "better predictions.\n",
    "\n",
    "This process of transforming/augmenting input features is called *feature engineering*\n",
    "- In Classical Machine Learning, it is the responsibility of the person fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In \"Classical\" Machine Learning (ML) the paradigm is to\n",
    "- *Manually* create a sequence of transformations from raw input to an alternate representation\n",
    "    - Feature engineering: create representations corresponding to \"concepts\" expressed by the data\n",
    "    - A sequence of transformations a *pipeline*\n",
    "- The final representation created by the sequence may result in a better prediction than that which could be obtained from the raw representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning: Introduction\n",
    "\n",
    "In Deep Learning, the paradigm is very similar to that of Classical Machine Learning\n",
    "- a sequence of transformations\n",
    "- resulting in a final representation better able to predict\n",
    "\n",
    "One difference from Classical ML is that a transformation is implemented by a *Neural Network*\n",
    "- The transformations are often organized into a sequential pipeline\n",
    "- The Neural Network implementing a transformation is called a *layer*\n",
    "- Deep Learning refers to a sequence of many layers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to Classical ML\n",
    "- the transformations of Deep Learning create alternate representations of the input that result in better prediction. \n",
    "\n",
    "**The key difference from Classical ML**:\n",
    "- the transformations of Deep Learning are \"discovered\"  rather than hand engineered\n",
    "    \n",
    "Here is a \"cartoon\" diagram of Deep Learning (as applied to the Classification task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center><strong>Layers</strong></center>\n",
    "    <br>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/W12_L1_NN_layers1920by1080.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A sequence of *layers* (vertical boxes)\n",
    "- Starting with the input layer (the large vertical box on the left)\n",
    "<div>\n",
    "    <br>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/Layers_W8_L2_Sl12_1.png\" width=20% align=\"left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Intermediate layer $\\ll$ takes as input the output of layer $(\\ll -1)$\n",
    "- Transforming it into an alternate representation\n",
    "\n",
    "<div align=\"middle\">\n",
    "    <br>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/Layers_W8_L2_Sl12_2.png\" width=40%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The output of the penultimate layer $(L-1)$\n",
    "is used as input to a final layer $L$ that implements either\n",
    "    - Classification\n",
    "    - Regression\n",
    "    \n",
    "<div align=\"right\">\n",
    "    <b>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/Layers_W8_L2_Sl13.png\" width=40% align=\"right\">\n",
    "    </center>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The process of moving through the layers from Input to penultimate is\n",
    "- Successive transformation of the input\n",
    "- Each layer's output is an alternate *representation* of the input\n",
    "\n",
    "<div>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/W12_L1_NN_layers1920by1080.png\">\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is similar to the Feature Engineering pipeline of Classical ML\n",
    "    - Implemented in many ML toolkits (e.g., `sklearn`)\n",
    "- Where the final version of the transformed data is fed into a Classifier/Regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will come to see:\n",
    "- Another **key difference** from Classical Machine Learning\n",
    "- Is that the **transformations we use in Deep Learning are non-linear**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks: introduction and notation\n",
    "\n",
    "The above was very informal.\n",
    "\n",
    "We need to introduce more concepts and, unfortunately, the notation that will carry us through\n",
    "the Deep Learning part of the course.\n",
    "\n",
    "Let's review our [Basic Notational standards](ML_Notation.ipynb)\n",
    "\n",
    "Time to go [under the covers of a layer](Intro_to_Neural_Networks.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What does a layer do ?\n",
    "\n",
    "As we saw, Fully Connected layer $\\ll$\n",
    "- Computes $n_\\llp$ features\n",
    "- Each feature $\\y_{\\llp,j}$ is the dot product of $\\y_{(\\ll-1)}$ and $\\W_{\\llp,j}$\n",
    "\n",
    "We would argue that the dot product is nothing more than pattern matching\n",
    "- $\\W_{\\llp,j}$ is a pattern\n",
    "- That layer $\\ll$ is trying to match against layer $(\\ll-1)$ output $\\y_{(\\ll-1)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this pattern/template matching intuition in mind, let's revisit our path through the layers.\n",
    "\n",
    "Let's start with the inputs to the NN: an example with our original features\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "    <br>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/Layers_W8_L2_Sl12_1.png\" width=30% align=\"left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Layer $1$ is attempting to match our input $\\x$ against $n_{(1)}$ patterns.\n",
    "\n",
    "So $\\y_{(1)}$ is a vector of synthetic features (an alternate representation of $\\x$)\n",
    "- where each feature in $\\y_{(1)}$\n",
    "is an indicator as to whether a particular pattern appears in $\\x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Layer $\\ll$ works similarly except that the patterns it matches are against synthetic feature in $\\y_{(\\ll-1)}$ rather than $\\x$.\n",
    "\n",
    "<div align=\"middle\">\n",
    "    <br>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/Layers_W8_L2_Sl12_2.png\" width=50%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ultimately, the $n_{(L-1)}$ synthetic features created by penultimate layer $(L-1)$\n",
    "- Is a representation of the input $\\x$\n",
    "- That has been sufficiently transformed\n",
    "- So that a Classifier/Regression model at layer $L$ can be successful\n",
    "\n",
    "<div>\n",
    "    <br>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/Layers_W8_L2_Sl13.png\" width=40% align=\"right\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize, our intuition is that\n",
    "- The first layer recognizes features (matches patterns) for *primitive* concepts\n",
    "- The second layer recognizes features that are *combinations* of primitive concepts (layer 1 concepts)\n",
    "- Layer $\\ll$ recognizes features that are *combinations* of layer $(\\ll-1)$ concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"middle\">\n",
    "    <center><strong>Features by layer</strong></center>\n",
    "    <br>\n",
    "    <!-- edX: Original: <img src=\"images/Layer_features.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/ThreeLayers_W8_L2_Sl21.png\" width=20%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is $W_\\llp$ ? Where did $\\Theta$ go ?\n",
    "\n",
    "Our old friend the dot product is back in the forefront.\n",
    "\n",
    "But now, the pattern matching is written\n",
    "$$\n",
    "\\W_{\\llp,j} \\cdot \\y_{(\\ll-1)}\n",
    "$$\n",
    "rather than the \n",
    "$$\n",
    "\\Theta \\cdot \\x\n",
    "$$\n",
    "which was familiar in the Classical Machine Learning part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unfortunately, this is an artifact of two different communities working independently\n",
    "- The Classical Machine learning community uses the term *parameters* and the Greek letter $\\Theta$\n",
    "- The Computer Science community uses the term *weights* and the letter $W$\n",
    "\n",
    "They are *exactly the same thing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, Neural Networks operate in layers\n",
    "- So only the dot product of the *first* layer involves $\\x$, which we have equated with $\\y_{(0)}$\n",
    "- The dot product of layer $\\ll$ is finding patterns in $\\y_{(\\ll-1)}$ rather than $\\x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Size of $\\W$: Always count the number of parameters !\n",
    "\n",
    "When constructing a layer of a Neural Network: **always count the number of weights/parameters. They grow quickly !!**\n",
    "\n",
    "$\\W_\\llp$\n",
    "- Consists of $n_\\llp = || \\y_\\llp ||$ units, each unit producing a new feature\n",
    "- Each unit performs the dot product \n",
    "$$\n",
    "\\y_{\\llp,j} = \\y_{(\\ll-1)} \\cdot \\W_{\\llp,j}\n",
    "$$\n",
    "- Each dot product thus involves $|| \\y_{(\\ll-1)} || +1 $ weights in $\\W_{\\llp,j}$\n",
    "    - the \"+ 1\" is because of the bias term in each unit\n",
    "- Thus the number of weights in $\\W_\\llp$ is $|| \\y_\\llp || * (|| \\y_{(\\ll-1)} || + 1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activation functions\n",
    "\n",
    "At this point perhaps you have a mechanical understanding of a neural network\n",
    "- A sequence of layers\n",
    "- Each layer is creating new features\n",
    "- Subsequent layers creating features of increasing complexity but transforming the prior layer's features\n",
    "- A new feature is created by a linear dot product followed by an non-linear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out that the non-linear activation function is *one of the keys* to Neural Networks !\n",
    "\n",
    "Let's explore [Activation functions](Neural_Networks_Activations.ipynb) in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final layer: Regression/Classification\n",
    "\n",
    "The Head Layer (layer $L$)\n",
    "- takes the final representation of the data\n",
    "- applies a layer determined by the task\n",
    "    - Regression\n",
    "    - Classification\n",
    "<div>\n",
    "    <!-- edX: Original: <img src=\"images/NN_Layers.png\"> replace by EdX created image -->\n",
    "    <img src=\"images/W12_L1_NN_layers1920by1080.png\">\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Regression is nothing more than a dot product\n",
    "    - $\n",
    "\\y_{L} = \\y_{(L-1)} \\cdot \\W_{L}\n",
    "$\n",
    "    - Implemented by a Fully Connected layer with **no** activation\n",
    "- Classification is nothing more than a dot product followed by a sigmoid activation\n",
    "    - $\n",
    "\\y_{L} = \\sigma( \\y_{(L-1)} \\cdot \\W_{L} )\n",
    "$\n",
    "    - Exactly the same way as discussed in our \"Classical\" Machine Learning lecture\n",
    "    - Implemented by a Fully Connected layer with a sigmoid activation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions to consider\n",
    "\n",
    "Some natural questions to ask at this point\n",
    "- How many layers should we have ( What is the right value for $L$) ?\n",
    "- How many units $n_\\llp$ should I have for each layer $1 \\le \\ll \\le (L-1)$ ?\n",
    "- What activation function should I use for each unit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will address each of these in the future.\n",
    "\n",
    "Perhaps the biggest question\n",
    "- $\\W_{\\llp,j}$ is the pattern used to recognize the feature created by unit $j$ of layer $\\ll$\n",
    "- How does $\\W_{\\llp,j}$ get set ?\n",
    "\n",
    "This will be the topic of the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "We will start to answer the question of how $\\W$ is determined.\n",
    "\n",
    "\n",
    "We will briefly [introduce training a Neural Network](Neural_Networks_Intro_to_Training.ipynb),\n",
    "a subject we will revisit in-depth in a later lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow: A toolkit for Neural Networks\n",
    "\n",
    "Why do we need a dedicated toolkit (Tensorflow) to aid the programming of Neural Networks ?\n",
    "\n",
    "It's mainly about the use of Gradient Descent in training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that a Neural Net (including one augmented by a Loss Layer) is doing nothing more than compute a function.\n",
    "\n",
    "Gradient Descent needs to take the gradient of this function (evaluated on a mini batch of examples) in order to update the weights $\\W$.\n",
    "\n",
    "There are at least two ways to obtain the Gradient\n",
    "- Numerically\n",
    "- Analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numerical differentiation applies the mathematical definition of the gradient\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\frac{ f(x + \\epsilon) - f(x) }{\\epsilon}\n",
    "$$\n",
    "\n",
    "- It evaluates the function twice: at $f(x)$ and $f(x+\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This can get expensive, especially since\n",
    "    - $\\x$ is a vector\n",
    "    - Potentially very long (e.g., many features)\n",
    "    - We need to evaluate the derivative of $f(\\x)$ with respect to each $\\x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Analytic derivatives are how you learned differentiation in school\n",
    "- As a collection of rules, e.g.,\n",
    "$$\n",
    "\\frac{\\partial (a + b)}{\\partial x} = \\frac{\\partial a}{\\partial x} + \\frac{\\partial b}{\\partial x}\n",
    "$$\n",
    "\n",
    "This is very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue in ordinary  code\n",
    "- The expression `(a + b) * c`\n",
    "- Is evaluated `tmp = a + b`\n",
    "- And the result passed to the next step of the computation, e.g, `tmp * c`\n",
    "- Losing the connection between `tmp` (a value) and the operation (plus) and addends (`a`, `b`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is no information recorded in ordinary code that would allow the application of analytic rules of differentiation.\n",
    "\n",
    "Tensorflow is different in that `(a + b) * c`\n",
    "- Is a symbolic expression (i.e., recorded as operation and arguments)\n",
    "- That is saved\n",
    "- Facilitating the application of analytic rules of differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We still write `(a + b) * c` but it really results in something like:\n",
    "- `tf.math.mult( tf.math.add(a, b), c)`\n",
    "\n",
    "The expression \n",
    ">`tf.math.add(a, b)`\n",
    "\n",
    "can be differentiated analytically because it records\n",
    "- the arguments are `a, b`\n",
    "- the operation is addition\n",
    "- we know the derivative of an addition operation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So Tensorflow facilitates analytic function differentiation while hiding the details from the user.\n",
    "- We will see some pseudo-code that shows how this is done\n",
    "- Check out the [Deeper Dive on Computation Graphs](Computation_Graphs.ipynb) if you want to know more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By the way: what is a Tensor ?  It is an object with an arbitrary number of dimensions.\n",
    "\n",
    "We use special cases all the time:\n",
    "- A scalar is a tensor of dimension $0$\n",
    "- A vector is a tensor of dimension $1$\n",
    "- A matrix is a tensor of dimension $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you've seen, we are already dealing with higher dimensional objects.\n",
    "\n",
    "Consider $\\y$:\n",
    "- $\\y_{\\llp, j, j'}$\n",
    "    - Output of layer $\\ll: \\y_\\llp$\n",
    "    - Unit $j$ of layer $\\ll: \\y_{\\llp, j}$\n",
    "    - Element $j'$ of the output of unit $j$ of layer $\\ll: \\y_{\\llp, j, j'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the future we will talk about *sequences* of $\\y$, thus adding another dimension: time.\n",
    "\n",
    "And, don't forget, the \"batch index\" dimension\n",
    "- Tensorflow processes *mini-batches* of examples, not singeltons\n",
    "- $\\y^\\ip_{\\llp, j, j'}$\n",
    "    - Element $j'$ of the output (given input examples $i$) of unit $j$ of layer $\\ll: \\y_{\\llp, j, j'}$\n",
    "\n",
    "The notation will become a little heavy but hopefully understandable as a way of indexing a high dimension object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequential versus Functional construction of a Neural Network\n",
    "\n",
    "The above description of a Neural Network imposed several restrictions on units (neurons)\n",
    "- Each layer is homogeneous\n",
    "- Layers are organized sequentially\n",
    "\n",
    "One can imagine a network graph without these restrictions\n",
    "- Not organized as layers\n",
    "- Pairs of units with arbitrary connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center>Sequential architecture</center>\n",
    "    <br>\n",
    "<img src=images/Sequential_arch.png>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center>Functional architecture</center>\n",
    "    <br>\n",
    "<img src=images/Functional_arch.png>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The layer-oriented network we described is called *Sequential* in Keras.\n",
    "\n",
    "The unrestricted network form is called *Functional* in Keras.\n",
    "\n",
    "For the most part, we will use Sequential networks\n",
    "- Easier to build\n",
    "- But less flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some \"Why's ?\"\n",
    "\n",
    "## What took so long: Preview\n",
    "\n",
    "An historical perspective:\n",
    "\n",
    "- Perceptron invented 1957\n",
    "- mid-1970's: First \"AI Winter\"\n",
    "- Late 1980's: second \"AI Winter\"\n",
    "- 2010: Re-emergence of AI\n",
    "\n",
    "The promise of AI led to great expectations, that were ultimately unfulfilled.\n",
    "The difficulty was the inability to train networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will defer a fuller answer to a later lecture.\n",
    "\n",
    "For now: seemingly minor choices were more impactful than imagined\n",
    "- Sigmoid as activation function turned out to be a problematic choice\n",
    "- Initializing $\\W$ properly was more important than imagined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Vanishing/Exploding Gradients\n",
    "    - problems arise when the gradient is effectively  0\n",
    "    - problems also occurs when they are effectively infinite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Computational limits\n",
    "    - It turns out to be quite important to make your NN big; bigger/faster machines help\n",
    "    - Actually: bigger than it needs to be\n",
    "        - many weights wind up near $0$, which renders the neurons useless\n",
    "        - [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)\n",
    "            - within a large network is a smaller, easily trained network\n",
    "            - increasing network side increases the chance of large network containing a trainable subset\n",
    "            - [summary](https://towardsdatascience.com/how-the-lottery-ticket-hypothesis-is-challenging-everything-we-knew-about-training-neural-networks-e56da4b0da27)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why do GPU's matter ?\n",
    "\n",
    "GPU (Graphics Processing Unit): specially designed hardware to perform repeated\n",
    "vector multiplications (a typical calculation in graphics processing).\n",
    "\n",
    "- It is not general purpose (like a CPU) but does what it does extremely quickly, and using many\n",
    "more cores than a CPU (typically several thousand).\n",
    "\n",
    "- As matrix multiplication is a fundamental operation of Deep Learning, GPU's have the ability to greatly\n",
    "speed up training (and inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Google has a further enhancement called a [TPU](https://cloud.google.com/tpu/docs/tpus) (Tensor Processing Unit) to speed both training and inference.\n",
    "-  highly specialized to eliminate bottlenecks (e.g., memory access) in fundamental Deep Learning matrix multiplication.\n",
    "\n",
    "Both GPU's and TPU's \n",
    "- Incur an overhead (a \"set up\" step is needed before calculation).\n",
    "- So speedup only for sufficiently large matrices, or long \"calculation pipelines\" (multiplying \n",
    "different examples by the same weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DL involves \n",
    "- Multiplying large matrices (each example) \n",
    "- By large matrices (weights, which are same for each example in batch)\n",
    "- Both GPU's and TPU's offer the possibility of large speed ups.\n",
    "\n",
    "- GPU's are **not** necessary\n",
    "    - but they are a **lot** faster\n",
    "    - life changing experience\n",
    "        - 30x faster means your 10 minute run (that ended in a bug) now only takes 20 seconds\n",
    "     - increases your ambition by faster iteration of experimental cycle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
