{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\E}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\o}{\\mathbf{o}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entropy, Cross Entropy, KL Divergence: deep dive\n",
    "\n",
    "The Cross Entropy cost function is omnipresent in Machine Learning\n",
    "as many problems involve matching probability distributions.\n",
    "\n",
    "Although not strictly necessary, it may be interesting to explore this concept more deeply.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <ul>\n",
    "        <li>Cross Entropy measures the \"distance\" between two distributions </li>\n",
    "            <li>You will often come across Cross Entropy and KL Divergence in  your future ML endeavours.</li>\n",
    "        </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entropy\n",
    "\n",
    "In order to understand *Cross* Entropy, we first try to explain *Entropy*.\n",
    "- A measure of the randomness of a distribution\n",
    "- Interpreted as the number of bits of information obtained from a single sample\n",
    "- an alternative to Gini for deciding which variable/what threshold to use in constructing a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a bit ?\n",
    "- Amount of information need to reduce uncertainty by a factor of 2\n",
    "\n",
    "\n",
    "### Example: what is a bit\n",
    "\n",
    "Here, a bit is the amount of information **not** a length of the message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simple case: two equally probable outcomes**\n",
    "\n",
    "Two outcomes: Rain, Sun\n",
    "$$    \n",
    "\\begin{array}{lll}\n",
    "\\text{Rain} & .5 \\\\\n",
    "\\text{Sun}  & .5\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Uncertainty about Rain is .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If I tell you that it will Rain (so $p_{\\text{Rain}}$ goes from $0.5$ to $1$)  the uncertainty is reduced\n",
    "by a factor of\n",
    "$$\n",
    "{1 \\over .5} = 2\n",
    "$$\n",
    "\n",
    "$\\log_2 (2) = 1$ \n",
    "\n",
    "so this conveyed 1 bit of *information* regardless of how many *physical* bits were sent.\n",
    "\n",
    "**Definition of a bit: Uncertainty reduction of a factor of 2**\n",
    "\n",
    "One bit reduces uncertainty ($1/p$) by a factor of $2$.\n",
    "\n",
    "$\\log_2 (1/p) = - \\log_2(p)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simple case: 4 equally probable outcomes**\n",
    "\n",
    "4 outcomes: Rain, Sun, Clouds, Drizzle\n",
    "$$    \n",
    "\\begin{array}{lll}\n",
    "\\text{Rain} & .25 \\\\\n",
    "\\text{Sun}  & .25 \\\\\n",
    "\\text{Clouds} & .25 \\\\\n",
    "\\text{Drizzle} & .25\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Uncertainty about Rain is .25.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If I tell you that it will Rain (so $p_{\\text{Rain}}$ goes from $0.25$ to $1$) I've reduced the uncertainty\n",
    "by a factor of\n",
    "$$\n",
    "{1 \\over .25} = 4\n",
    "$$\n",
    "\n",
    "This is $- \\log_2 (.25) = 2$ bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Outcomes with arbitrary probabilities**\n",
    "\n",
    "Two outcomes with unequal probability: Rain, Sun\n",
    "$$    \n",
    "\\begin{array}{lll}\n",
    "\\text{Rain} & .25 \\\\\n",
    "\\text{Sun}  & .75\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If I tell you that it will Rain (so $p_{\\text{Rain}} = 1$) I've reduced the uncertainty\n",
    "by\n",
    "$$\n",
    "{1 \\over .25} = 4\n",
    "$$\n",
    "\n",
    "so this is $\\log_2 (2) = 2$ bits of information.\n",
    "\n",
    "If I tell you that there will be Sun (so $p_{\\text{Sun}} = 1$) I've reduced the uncertainty\n",
    "by\n",
    "$$\n",
    "{1 \\over .75}\n",
    "$$\n",
    "\n",
    "so this is $- \\log_2 (0.75) = 0.415$ bits of information.\n",
    "\n",
    "In this case, the number of bits I convey to you is different depending on the message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average number of bits in a sample\n",
    "\n",
    "The average number of bits (*information*) in a sample randomly drawn from the distribution is\n",
    "\n",
    "$\n",
    "H_i = -\\sum\\limits_{k=1 \\atop p_{i,k} \\ne 0}^{n}{{p_{i,k}}\\log_2(p_{i,k})}\n",
    "$\n",
    "\n",
    "For the 75%/25% case above\n",
    "\n",
    "$\n",
    "H_i = 0.75 * 0.415 + 0.25 * 2 = 0.81\n",
    "$\n",
    "\n",
    "25% of the time I give you 2 bits of information; 75% of the time only 0.415 bits \n",
    "\n",
    "\n",
    "This is also called the **entropy**\n",
    "\n",
    "It measures the unpredictability of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entropy: a measure of randomness of a distribution\n",
    "\n",
    "Consider a *pure distribution*: one in which all probability is concentrated at a single value $k$\n",
    "\n",
    "$$\n",
    "p_{i,k} = 1\n",
    "$$\n",
    "\n",
    "A pure distribution has 0 entropy  since $\\log(p_{i,k}) = 0$\n",
    "\n",
    "On the other hand, consider a (binary) distribution in which each outcome has equal probability ($50 \\%$)\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "H_i  & = & -\\sum\\limits_{k \\in \\{0,1\\}}{{p_{i,k}}\\log_2(p_{i,k})}\\\\\n",
    "& = & -\\sum\\limits_{k \\in \\{0,1\\}}{ 0.5 \\log_2(0.5)}\\\\\n",
    "& = & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So Entropy is a proxy for non-uniformity of a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross entropy, KL Divergence\n",
    "\n",
    "Whereas Entropy measured the randomness of a single distribution,\n",
    "Cross Entropy measures the \"distance\" between two probability distributions $p$ and $q$.\n",
    "\n",
    "We will call $p$ the *true* distribution and $q$ the estimated distribution.\n",
    "\n",
    "\n",
    "**Cross entropy between two discrete probability distributions $p$ and $q$:**\n",
    "\n",
    "$ H(p, q) = -\\sum\\limits_{x}p(x) \\log q(x) $\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to Entropy interpretation as weighted sum of bits but \n",
    "- uses true probability $p$ as weights\n",
    "- uses estimated probability $q$ to measure the bits\n",
    "\n",
    "When estimated distribution $q$ is a prediction $\\hat{\\y}$ produced by a model\n",
    "- Cross Entropy is a proxy for the difference between the true distribution $p$ and the model distribution $q$\n",
    "- Our Classification Loss Function seeks to minimize this difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If distributions $p(x), q(x)$ are the same\n",
    "- cross entropy equals entropy: $H(p,p) = H(p)$\n",
    "\n",
    "The difference \n",
    "$(H(p,q) - H(p))$ is called the *Kullback-Leibler* or KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside**\n",
    "\n",
    "There is another interpretation of Cross Entropy.\n",
    "\n",
    "We've seen that the amount of information depends on number of bits of each outcome $k$: $\\log_2(p_{i,k})$.\n",
    "\n",
    "$ H(p, q) = -\\sum\\limits_{x}p(x) \\log q(x) $\n",
    "is the average number of bits if information is given by $q$\n",
    "\n",
    "If the number of bits  is different than $\\log_2(p_{i,k})$\n",
    "- then this message $q$ is *optimal* in length\n",
    "    - only if the distribution is some other $q_{i,k}$\n",
    "    \n",
    "The physical length will equal the information content only if $p(x) = q(x)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
